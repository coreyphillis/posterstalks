<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<SearchIndexes Version="1.0">
    <Documents>
        <Document ID="10">
            <Title>DataONE</Title>
            <Text>DataONE is a large consortium grant aimed at building the infrastructure needed to maintain and share environmental data. Any organziation can apply to become a member node. So as data are generated, they can be pushed into a member node, get a DOI, and then become available to anyone with rights to such data. By default, the user is the only one who can access it. But over time time, this can be extended to the lab group, the organization, and everyone.

R fits into DataONE in two ways.

A) There is now an R Package in the works, called d1_R that allows users to first authenticate, and then query and obtain data directly into R. This means that one could share an R script with anyone and therefore be able to submit those in as well.

B) As part of a working group called the Provenance working group, I have recently begun implementing a formal way to capture workflows in R. Just like with knitr, as long as a user is able to identify processing steps (which would be functions), data, and parameters, then R quietly generates a workflow trace for every run and can also keep those in. So results from various runs could potentially be compared, perhaps visually, to see differences among runs and what factors (inputs, or function parameters) generated such differences. Rprov is a new project and if anyone else is interested in the possibilty of contributing or collaborating, please do get in touch.

</Text>
        </Document>
        <Document ID="15">
            <Title>gcb2693</Title>
            <Text>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼Global Change Biology (2012) 18, 2102–2110, doi: 10.1111/j.1365-2486.2012.02693.x REVIEW
Advances in global change research require open science by individual researchers
ELIZABETH M. WOLKOVICH*†, JAMES REGETZ‡ andMARY I. O’CONNOR†
*Department of Biological Sciences, University of California, San Diego, 9500 Gilman Drive #0116, La Jolla, CA 92093-0116,
USA, †Department of Zoology, University of British Columbia, 6270 University Boulevard, Vancouver, BC V6T 1Z4, Canada, ‡National Center for Ecological Analysis and Synthesis, 735 State Street, Suite 300 Santa Barbara, CA 93101, USA
Abstract
Understanding how species and ecosystems respond to climate change requires spatially and temporally rich data for a diverse set of species and habitats, combined with models that test and predict responses. Yet current study is hampered by the long-known problems of inadequate management of data and insufficient description of analytical procedures, especially in the field of ecology. Despite recent institutional incentives to share data and new data archiving infrastructure, many ecologists do not archive and publish their data and code. Given current rapid rates of global change, the consequences of this are extreme: because an ecological dataset collected at a certain place and time represents an irreproducible set of observations, ecologists doing local, independent research possess, in their file cabinets and spreadsheets, a wealth of information about the natural world and how it is changing. Although large-scale initiatives will increasingly enable and reward open science, we believe that change demands action and personal commitment by individuals – from students and PIs. Herein, we outline the major benefits of sharing data and analytical procedures in the context of global change ecology, and provide guidelines for overcoming common obstacles and concerns. If individual scientists and laboratories can embrace a culture of archiving and sharing we can accelerate the pace of the scientific method and redefine how local science can most robustly scale up to globally relevant questions.
Keywords: code management, data management, global change ecology, open science, scientific method Received 19 December 2011; revised version received 22 February 2012 and accepted 24 February 2012
Introduction
In 1953, a group of pediatric cancer specialists met to discuss the process and progress of scientific research in their field (Unguru, 2011). At the time, the survival rate for the most common childhood cancer, a type of leukemia, was &lt;4%. The result of the meeting was a transformation of cancer research in the United States: from small scale, local laboratory work to team science and cooperative research. In 1954 multi-institutional studies combined with strong data sharing policies began uniting the research efforts of 40 hospitals (Ung- uru, 2011), and the cure rate started to climb. Today, the survival rate for the same childhood cancer is 94% and the field worldwide recognizes cooperative, collab- orative science as a driving force behind this change (Devidas et al., 2010).
In the same year, cooperative cancer research began, botanist R. S. R. Fitter began to collect what would become a 47-year record of flowering times in his local
Correspondence: Elizabeth M. Wolkovich, tel. + 604 822 0862; fax + 604 827 5350, e-mail: wolkovich@biodiversity.ubc.ca
area (Fitter &amp; Fitter, 2002). Today, Fitter’s dataset repre- sents one of the most important records of how plant species have shifted with climate change: researchers have used it further to show how climate responses are linked to trait conservatism and species distributions (Davis et al., 2010; Hulme, 2011). Such wide use is in no small part due to the fact that Fitter and his co-author took the unusual step of publishing the dataset with their article in Science (Fitter &amp; Fitter, 2002), freeing others to use the data.
This example is inspiring. Unfortunately, it is outside the norm for global change ecology, where – compared with other fields working toward data sharing (Scho- field et al., 2009; Igo-Kemenes, 2011; Overpeck et al., 2011) – progress has been unusually slow. Despite over 15 years of focused efforts toward developing infra- structure for sharing data, in community ecology and global change ecology participation by researchers today remains the extreme exception (Michener et al., 1997; Fegraus et al., 2005; Parr &amp; Cummings, 2005; Nel- son, 2009; Reichman et al., 2011). Yet anthropogenic environmental change makes most ecological field data collected effectively irreproducible, critical snapshots in
2102
© 2012 Blackwell Publishing Ltd
OPEN SCIENCE IN GLOBAL CHANGE RESEARCH 2103
￼￼￼Research at height of its potential
Models guide data collection
Data tests and improves models
￼￼Lack of data and models hinders progress
￼Models
Figure 1 Accelerating research on applied issues requires high availability of both data and models. Adapted from Holling (1978).
￼￼￼time that cannot simply be remeasured. Ecologists now realize studying ‘Nature’ – a pristine entity – is an impossibility (Sih et al., 2004). Thus, every dataset has potential value for synthetic projects that explore ecological change in space and time.
Rapid and major advances in global change research are supported by high data and model availability (Fig. 1), which requires a shift toward open science at the individual level. Herein, we argue that individual scientists can have a tremendous, positive impact on the pace of global change science by archiving and sharing their data along with any relevant data processing steps and analytical procedures (hereafter referred to simply as ‘code’ in reference to computer source code, although reproducible steps can be captured in non-code forms as well). First, we provide a brief overview of how local, ecological data is used in global change science. We then argue that the top-down initiatives for data- and code sharing already underway (Mervis, 2010; Whitlock et al., 2010) are necessary, but not sufficient: individual scientists must actively participate in open science for it to succeed. We outline simple steps that individuals can take in their careers to overcome common obstacles and develop the necessary skills to archive and share data and code, with the goal of moving data and code from the individual to public level. Such a shift will benefit individual ecologists and research groups, as well as whole fields of research, with cascading effects for the contribution of ecological research in the sciences and in ecosystems across the globe.
Poor datasharing slows global change research
Across fields, papers and initiatives building on open data access have led to breakthroughs and critical
insights (Piwowar et al., 2008; Carpenter et al., 2009). In medicine, collaborative team science, including shared data and resources, has led to large improvements in cancer treatments (Reaman, 2002) and in the early diag- nosis of Alzheimer’s disease (Kolata, 2010; August 12). In ecology, shared data has underlied recent advances in understanding global ecosystem services, such as decomposition (Wall et al., 2008) and pollination net- works (Rezende et al., 2007). In ocean sciences, the invention and advancement of large-scale plankton monitoring, begun by one scientist, has documented major shifts in ocean food webs and algal blooms over recent decades (Edwards &amp; Richardson, 2004; Edwards et al., 2010). To date, the data – which are publicly available – have resulted in over 1300 publications (Sir Alister Hardy Foundation for Ocean Science, 2012).
In global change research, synthesis efforts have been critical to showing how climate change impacts ecologi- cal systems (Root et al., 2003; Rosenzweig et al., 2008), with long-term time-series data (Hegerl et al., 2010) act- ing as the standard for detecting and attributing biolog- ical shifts to climate change. Yet very few of these datasets are available, limiting our ability to answer even basic questions, such as how frequently climate change impacts are actually observed (Parmesan &amp; Yohe, 2003). Further, the iterative process of testing and fitting models – conceptual, theoretical or simulation – to data can advance only as fast as data availability and model development time allow (Fig. 1). We argue that global change research, especially in the field of ecol- ogy, is stymied by inefficient data use and low-model availability, hindering research applications. Alongside an urgent need to collect new data to answer global change questions, advances can be made by synthesiz- ing existing data to document historical change and baseline conditions.
Despite recent progress at the level of institutions and the development of new infrastructure for data archiving (Reichman et al., 2011), access to ecological data is still extremely low (Nelson, 2009), resulting in a grossly impeded pace of research as synthesis projects require extensive emails and extraction of data by hand from publications. In addition to being time intensive, data extraction from publications usually yields only derived data (e.g., treatment means and standard errors), which confines researchers to a narrow set of meta-analytic tools that handle such data types (Cooper et al., 2009). In contrast hierarchical models based on raw data allow more robust tests and provide not only an understanding of effect sizes but also comprehen- sive information on the error structure of the data. These methods, thus, can provide novel insights and improve future experimental designs; not surprisingly, much research in the medical sciences has already
Open data
Open code
© 2012 Blackwell Publishing Ltd, Global Change Biology, 18, 2102–2110
Data
2104 E. M. WOLKOVICH et al.
￼￼5% lost data
￼9% could not find working email
￼￼￼5% required authorship
￼57 datasets inquired for access
65 datasets identified for analysis
￼￼￼64% responded
￼￼￼14% declined to share
￼￼￼30% shared data
8 publicly-available (all LTER)
￼￼46% expressed interest but did not share
￼Figure 2 For a recent meta-analysis on the effects of temperature on plant phenology one author (EMW) identified 65 datasets of inter- est (a nearly equal mix of observational long-term data and experimental results) and attempted to obtain the raw data for them. For eight datasets data were already publicly available online (all through the US Long-Term Ecological Research program), for the remain- ing 57 the author sent over a 100 emails to request data: 64% of emails received a response, 27% did not (for 9% of emails multiple email addresses failed). Of those responding, 30% shared data.
￼￼￼￼￼￼￼switched to this method for meta-analyses (e.g., Alberti et al., 1995; Concato et al., 2000; Baigent et al., 2005).
In global change ecology, however, raw data are rarely available. For a recent meta-analysis one of the authors (EMW) attempted to track down raw experi- mental and long-term monitoring datasets (Fig. 2). After >100 emails inquiring for nearly 60 datasets over 10 months, only 11 datasets were eventually shared. Yet well over half of those who responded expressed interest in sharing their data.
What prevented the large group of researchers who expressed interest in sharing from following through? Although we do not have information on this sample, we can speculate that the reasons are similar to those addressed in several recent reviews of perceptions of the risks of data sharing, and the realities (Table 1, see also Parr &amp; Cummings, 2005; Smith, 2009; Overpeck et al., 2011; Tenopir et al., 2011; Whitlock, 2011). Such study suggests many common concerns may be based more on fears than reality and that sharing can have many benefits for the sharer. For example, papers accompanied by shared data are cited more often (Piwowar et al., 2007; Zimmerman, 2008). Further, many common issues, such as attribution and credit for data, are already being tackled by organizations and initiatives focused on promoting and providing a means to share data (Tables 1 and 2).
More persnickety issues, however, remain. Open sci- ence means increased collaboration at numerous stages in the process – data sharing, code sharing and idea sharing – and concerns have been raised over whether such sharing and collaboration are properly credited by traditional citation metrics and methods (Raff, 2003;
Kueffer et al., 2011). Properly crediting data is a real issue, and one that – given more synthetic and large- scale research – will need to be addressed with cultural and institutional changes. Ideally, researchers repur- posing data will involve data holders throughout the project – from hypothesis refinement to publication – since data holders usually have additional insights and understanding of their data that is difficult to capture in metadata. How to involve data holders on very data-rich projects (i.e., those projects that use tens or hundreds of individual datasets) requires additional consideration. Authorship-in-exchange-for-data poli- cies may discourage many researchers from using all available datasets. In addition, such policies have become problematic in cases of blame-assignment when issues with a study arise (Kennedy, 2003). Fur- ther issues could arise if data-authors do not agree with findings and thus pull data from a particular study. Although some have argued for inclusive authorship policies (Kueffer et al., 2011), many guidelines suggest data alone does not yield authorship (Weltzin et al., 2006). More large-scale, data-reuse projects in global change ecology are probably required before commu- nity agreement on new policies will be possible. Even- tual guidelines, however, will have to balance the demands of crediting the valuable resource of data, while not hindering research. At this point, we believe authorship considerations must be handled on a case- by-case basis.
Another concern is whether data sharing creates a system where ‘cheaters’ – those who do not share data, but copiously use public data – will reap the greatest benefits. Although this is possible according to basic
© 2012 Blackwell Publishing Ltd, Global Change Biology, 18, 2102–2110
Table 1 Common concerns associated with sharing data and code and responses to those concerns
closely with those who are directly involved in the data collection. The exceptions typically involve broad data synthesis and re-analysis activities for which the spe- cific research questions are well beyond the scope of what could be answered by any individual data collec- tor. Moreover, those who use existing data generally also make their own data available (Zimmerman, 2008). At a more global level, shared data can grossly benefit scientists from under-represented countries, providing them with access to large datasets they could never develop empirically given local funding levels. At the level of the scientific method, data synthesis does not slow data collection but rather, guides and encourages it (Fig. 1).
Recent events, such as those surrounding stolen emails from the University of East Anglia, have made it clear that improved descriptions of data manipulation and analysis steps must be a part of this new paradigm of open science. Increasingly, global change science is indirect (Mesirov, 2010), that is, datasets are too large to visually handle and thus researchers must process data without seeing every row personally. Revealing one’s potentially imperfect and inelegant computer code and related details may be uncomfortable at first, but the community must accept that computational decisions should be subject to review and refinement much like the rest of our science (Ince et al., 2012). With greater openness, the opportunity to more rapidly converge on better analytical methods is immense (Raymond, 1999). As with data, the research community will benefit from increased opportunity to reuse and extend shared code, or to collaborate in enhancing the features of commu- nity models. Expectations of shared code, as well as data, also level the playing field of shared resources: those re-purposing data often have invested large amounts of time in developing and testing sophisti- cated code, from which others, including data collec- tors, could benefit. Further, openness with analytical code would make model comparison a less-daunting task, and would engage researchers who may not be interested in developing models from scratch, but who could improve or apply such models. Code sharing would also allow routine tests of one hypothesis across multiple systems and would remove some of the road- blocks to interdisciplinary work. Ultimately, it would fundamentally shift the pace of global change research, aiding progress toward science that operates at its highest potential (Fig. 1).
Solution: individual commitment to sharing at the grassroots level
Open science will not progress until researchers recog- nize that every dataset can make multiple contributions
OPEN SCIENCE IN GLOBAL CHANGE RESEARCH 2105
￼Perceived concern
Insufficient attribution and credit
Uneven playing field between those who share data and those who only use shared data
Risk of revealing sensitive information (e.g., harvest data, rare species locations)
Lost opportunity to publish on one’s own data
High financial costs
High personal time costs
Potential for data misuse
Fear of exposing imperfect coding practices
No mechanism for non-programmers to share analyses
Concern about longevity of repositories
No personal benefit to sharing
Reality
Increased citation of paper with data; publication credit for data (e.g., Ecological Archives, Dryad DOIs)
Promotes equality on some scales (international researchers, those without funding to collect data, etc.); first-hand knowledge of data will remain invaluable
Many options available to scramble information
No need to post all long-term data; e.g., with 30 years of data, publishing years 1–10 makes it easier for you (or someone else) to post the rest at a later date (Roberts, 2002)
Overall costs are quite low (Piwowar et al., 2011), but funding agencies should consider funding data management costs in proposals
There are a variety of options now to post data – some require minimal time, although greater investment yields greater utility to you and others (Borer et al., 2009)
Rarely – if ever – reported, further, careful metadata should mitigate this (Whitlock, 2011)
‘Publish your code: it is good enough’ (Barnes, 2010)
Visual workflow software: Kepler, Taverna, and VisTrails
(see also Table 2)
Even with uncertain long-term futures, initiatives like DataONE will certainly increase lifespan of data relative to ad hoc individual data archiving practices
Using repositories relieves you of the hassle of managing/archiving your own files; access your content from anywhere; emerging automated tools will allow further benefits,
e.g., automated taxa scrubbing, easy integration with other online datasets
￼￼game-theory, in reality the outcome of open science is more likely to involve mutually beneficial partnerships, rather than parasitism. Most ecologists who work with existing data recognize the value of collaborating
© 2012 Blackwell Publishing Ltd, Global Change Biology, 18, 2102–2110
2106 E. M. WOLKOVICH et al.
Table 2 Actions and related skills needed for data and code management and sharing. Because many articles and online sites are devoted to this topic we review skills and resources here briefly
￼Action
Archive and share data in-house (Step 1)
Archive and share code in-house (Step 1)
Archive and share data publicly (Step 2).
Archive and share code publicly (Step 2)
Use and cite existing public data when possible (Step 3)
Include data and code management skills when teaching and mentoring (Steps 1 and 3)
Related skills
Data management, metadata creation (Borer et al., 2009)
Workflow management, from the field/ laboratory to analysis (Borer et al., 2009)
Data management, metadata creation
Workflow management, from the field/ laboratory to repository
Awareness of relevant data repositories; ability to use relevant data query interfaces and tools
Data management, workflow management (Borer et al., 2009; Whitlock, 2011)
Advantages
Data preserved for individual researcher to reuse and share with colleagues and students
Analyses archived for individual researcher to re-run, reuse, and share with colleagues and students
Data can be used to answer unexpected questions; satisfy funder or journal data policies; redundant backup
Analyses/models can be reused by others; definitive statement of what you did; opportunity for feedback, improvement, collaboration
Preliminary data to test and refine hypotheses available to all; encourages beneficial collaboration, synthesis, and sharing
Promote a new culture of openness
Example tools/services
Data management desktop software (Morpho1); metadata standards (EML2, FGDC CSDGM3, Darwin Core4);
Scripted languages (R5, MATLAB6, Python7); version control software (subversion8, Git9); scientific workflow software (Kepler10, Taverna11)
DateONE12 member nodes (KNB, Dryad13, Mercury14); PANGAEA15, NCDC16
Data repositories that accommodate code (KNB); workflow repositories (myExperiment17); code hosting services (GitHub18)
DataONE,
GBIF19, NASA GCMD20
DataONEpedia21, Software Carpentry22
￼￼1http://knb.ecoinformatics.org. 2http://knb.ecoinformatics.org/software/eml. 3http://www.fgdc.gov/metadata/csdgm. 4http://www.tdwg.org/activities/darwincore. 5http://www.r-project.org. 6http://www.mathworks.com/products/matlab/index.html. 7http://python.org.
8http://subversion.apache.org. 9http://git-scm.com. 10http://www.kepler-project.org. 11http://www.taverna.org.uk. 12https://www.dataone.org. 13http://datadryad.org. 14http://mercury.ornl.gov/ornldaac. 15http://www.pangaea.de. 16http://www.ncdc.noaa.gov/paleo/. 17http://myexperiment.org. 18http://myexperiment.org. 19http://data.gbif.org. 20http://gcmd.gsfc.nasa.gov. 21https://www.dataone.org/dataonepedia. 22http://software-carpentry.org.
to scientific understanding. First, the dataset can serve dataset represents a spatial, temporal or taxonomic the original intent of the researcher to address an replicate potentially useful in future synthetic projects immediate or local question or hypothesis. Second, the likely not anticipated at the time of original data
© 2012 Blackwell Publishing Ltd, Global Change Biology, 18, 2102–2110
collection. For example, long-term data and re-surveys of species’ phenologies and distributions – collected originally for a variety of unrelated purposes – have underlied most evidence of biotic responses to climate change (Parmesan et al., 1999; Root et al., 2005; Walther et al., 2005). Similar extensions of data have allowed global assessments of marine ecosystems (Halpern et al., 2008) and a diversity of paleoecological studies have shown how species’ ranges shift with climate (Jackson et al., 1997; Gray et al., 2006). These unantici- pated uses often manifest in collaborative research years after the data were originally collected. With these ultimate uses in mind, traditional methods of data storage and curation are outdated and often impede achieving the second goal.
Although many global change ecologists may see the merits and have interest in data sharing (Fig. 2), the apparent lack of rewards for individuals and a rampant lack of training in good practices likely limit most. While initiatives from funding agencies, societies and journals are underway (Stein, 2008; Mervis, 2010; Whit- lock et al., 2010) and may have great power in altering the reward structure, we believe individuals have the greatest power for change. Further, the resources for individuals to effectively manage data and code are already in place. Herein, we describe a three-step approach that outlines how individual researchers can identify and develop the skills needed and actions they can take now to promote data and code sharing in their community. We note that commitment toward better management is the main needed action; exact resources to use and how much training toward the most advanced data and code management researchers decide to pursue will vary (Table 2).
Step 1. Archive and share data and code in-house
Managing data and code so that it can be useful to researchers months and years later can seem painful and tedious when done as an afterthought. However, by considering the initial use and longer term steward- ship of data and code early in a project researchers can establish a foundation for effective management and in-house sharing.
Researchers should consider their data’s life cycle (Lee et al., 2009; Strasser et al., 2011) – how will data collected in the field or laboratory and its documenta- tion (metadata) be organized initially and in computer format? How will it be backed-up and where will it be stored eventually? Most researchers accomplish ele- ments of this process easily as part of their scientific process. For many, the needed advance is to provide documentation and long-term storage. Excellent meta- data, for example, usually requires centralizing only a
few key pieces of information: when, where, and how data were collected, and by whom. Such information generally takes only a few minutes to include if done alongside data entry, but because it is not a process ecologists are generally trained in, it is often skipped. This first step, however, of providing basic metadata overcomes a major challenge to future effective use of the data. The need for good data curation has long been recognized and steps toward this goal nicely outlined by Borer et al. (2009). While sometimes seen as tedious, including more detailed metadata about the the physi- cal format and structure of the data (e.g., number of col- umns and rows, data types and units) ensures that future downstream analyses can be performed with less risk of misinterpretation.
Researchers also need to move toward more deliber- ate consideration of their planned analyses, spanning all steps of an analytical workflow that transforms raw input data to final results. What tests will be run on what subsets of the data? What program will be used? At the outset, mentally conceiving of this process in the form of a flow diagram can yield a more coherent plan for how best to organize data and, with practice, can streamline and accelerate analyses (Goodchild, 2000). Particularly for synthetic projects where data are trans- formed or incorporated into a summary statistic, docu- mentation of the flow from data to manuscript figures and models is invaluable (Ellison, 2010). Such docu- mentation not only ensures the reproducibility of scien- tific work but also allows others to build on the work by expanding datasets or adapting analyses to different situations and research questions. Documentation of analytical workflows can take several forms. Scripted languages (such as R, MATLAB, or Python) are ideal because they allow for a written file that, when run, can read in the data and carry out all relevant computations – including creation of figures and tables – without error-prone manual intervention. This approach is advantageous because no additional documentation is required and it has direct benefits to the individual: complex code components can be reused, and manu- script revisions are far easier when the analytical pro- cess can be re-run identically and rapidly. A complete history of script modifications, and the final versions themselves, can be easily managed using version con- trol software that tracks, dates, and documents changes to the analysis process (Table 2). Alternatives to script- ing languages include workflow software, which is freely available to help track both data management and analytical workflows (Table 2).
Following these practices in a research group or a particular research program has clear benefits. First, researchers at different career stages can learn together how to use software, to create useful metadata, and
© 2012 Blackwell Publishing Ltd, Global Change Biology, 18, 2102–2110
OPEN SCIENCE IN GLOBAL CHANGE RESEARCH 2107
2108 E. M. WOLKOVICH et al.
archive details of data collection and analysis properly. Research groups can discuss issues in developing their data- and code-curating procedures, and by engaging members of a group with particular strengths, all mem- bers can learn what works well. Finally, research groups often perform research on one or several related themes, with students working on similar species, ques- tions or sites. The potential for synthetic projects within research groups is great, if data and code from past stu- dents and postdocs are clearly archived and available to future students. The training and research opportu- nities for students and PIs associated with good data and code-curating practices are clear incentives to spend some time on this issue in laboratory meetings and student training.
Step 2. Archive and share data and code publicly
Once good practices are developed within a research group, making data public is actually quite simple. Thus, researchers who adeptly manage data and code in-house are well poised to make data public, as required by many new grant agencies’ and journals’ requirements (Mervis, 2010; Whitlock et al., 2010). Data can be made public at different levels: full datasets can be archived with journals or data repositories (Table 2). Or, metadata alone (without the full dataset) can be posted to some repositories (e.g., KNB), leaving distri- bution of the full dataset under the control of the researcher. Posting metadata alone at an early stage makes posting data when it is finally available a less- daunting task. Researchers should, however, ideally post raw data and make them freely available with use- ful metadata and accompanying processing and analy- sis scripts; if posting raw data is not possible due to collaborator restrictions, researchers should supply derived data products (e.g., means, sample sizes, and standard errors). In all cases, laboratory groups should create metadata in a format endorsed by community- sanctioned repositories (e.g., KNB, Dryad, and Mercury) so posting data is simple and direct.
Step 3. Engage in a culture of open science
Individual researchers currently have myriad opportu- nities to promote and influence the process of data- and code sharing, and associated collaborations, in global change ecology. They can simply engage in discussions on data sharing to help others see the potential to advance science if data and code were managed and shared more thoughtfully. They can review data man- agement plans in proposals carefully, pointing out issues and suggesting improvements whenever possi- ble. We especially believe including data and code
management skills as a part of science when teaching and mentoring can help shift the practice of science. This is a particularly important option for funded pro- jects where teaching is an integral goal (such as the US National Science Foundation’s IGERT program and training initiatives at large synthesis centers).
Looking forward: progress toward a new mode of science
Improving success of top-down initiatives through grassroots support
Increased recognition of the possibilities data and code sharing provide (Fig. 1) can, in turn, create greater opportunities for successful larger data and code initia- tives. Other fields have shown that researcher support and request for top-down actions in data sharing can create sweeping changes in the practices of individual researchers. Journals require genetic data to be rapidly and freely available in central repositories (Field et al., 2009; Schofield et al., 2009) and we suggest ecological and global change journals should follow the recent suite of evolution journals (Whitlock et al., 2010) that now require data related to articles to be posted to pub- lic repositories (Savage &amp; Vickers, 2009) and encourage analytical workflow posting as well. Societies and agen- cies can support such efforts by providing standards of practice for data (file types, suggested repositories) and code (scripting practices and how to capture analytical workflows in non-scripted computer programs) and by offering training sessions or workshops – for example in association with professional societies’ annual meetings.
We believe funding agencies have great power to enact change. The European Commission’s major fund- ing program, FP7, requires data management plans of proposals and Germany’s Research Foundation (Deut- sche Forschungsgemeinschaft, DFG) now requests plans for facilitating reuse of data, with certain disci- plines requiring formal data management plans. In the US, the National Science Foundation (NSF) recently required a data management plan for all research pro- posals. We hope these new requirements signal a changing tide in the support and credit given for data in the sciences. Assessment of such data management plans should make usable (i.e., provided with complete metadata), public data a priority. Researchers should suggest how they will manage data and metadata from the field to the public repository, and include informa- tion about the total amount of data expected to be archived and when. In the future, we hope national and international funding agencies will review results from previous support with an eye toward both the quality of public data – as well as the publications – produced.
© 2012 Blackwell Publishing Ltd, Global Change Biology, 18, 2102–2110
In the US, reviewers of NSF proposals now have the ability to think about, critically review, and reward excellent data management practices of their peers.
Scaling up research: new skills and roles
We have attempted here to outline the argument for and path toward data and code sharing by individual global change scientists: starting a grassroots effort to promote a new perspective and standard for how we value and use global change ecology data in the scien- tific process (Fig. 1). We believe our above action items implemented in just a small fraction of global change laboratories could rapidly advance our understanding of fundamental issues in global change research. There- upon, future initiatives can build toward a new mode of science, one that can tackle questions on new scales and deliver verifiable research for policymakers.
As data and code become more widely available, scaling up the scientific process may help reshape attri- bution. Global change ecologists are often adept at working across thematic scales – from populations to communities, for example – highlighting a skill in developing innovative ideas and concepts. Yet many ecologists also possess skills at well-designed data col- lection and creative analysis. Currently, however, these skills are not given adequate recognition even though they are critical to first rate science (Fig. 1). In the future, we hope the field of global change ecology will find avenues to give suitable credit to those whose spe- cialized skills have advanced science by fostering col- laboration, cultivating valuable intradisciplinary and interdisciplinary teams or by providing high quality, reusable data and analyses. We suspect such a cultural transformation is underway, but we hope our proposal to shift how global change biologists approach data- and code sharing will broaden and accelerate our field’s thinking about the best integration of skills, data, credit and rewards.
Conclusions: the future of data in global change ecology
Sharing of data and code does not imply that new data collection will ever become less useful or necessary, but rather that data and code sharing can both advance how useful and insightful new data collection and models are. Ecology today is still most often carried out at local scales (‘boots and buckets’), and such study continues to generate observations and information fundamental to documenting and understanding change from the individual organism to the community and ecosystem levels. Insights from this level of science need to be better integrated with insights from large-
scale database and model efforts, and conversely, large- scale patterns must be validated and informed by detailed local processes as well.
Today global change ecology has an opportunity for transition. While small-scale local research continues to be critical, projects are increasingly conducted less on local scales (Stokstad, 2011) and more at the regional, national, and global level (e.g., Nutrient Network, NEON). At the same time, climate-induced shifts in organisms worldwide have enhanced public interest in ecological research. This is in large part because of the enormous implications for biodiversity and ecosystem services that in turn can impact basic human needs such as access to water or food. The increasing rele- vance of research in global change ecology, along with developments in infrastructure that enable distributed storage of, access to, and analysis of data, provide tre- mendous opportunity to transform how ecologists ‘do science’ – with greater openness benefiting individuals and the field as a whole.
Acknowledgements
The authors would like to thank J. M. Wolkovich for initial information on data sharing in oncological research, M. Jones, B. Leinfelder, M. Schildhauer, M. Whitlock and three anony- mous referees whose comments on earlier drafts improved this manuscript. This work was conducted while EMW was an NSF Postdoctoral Fellow in Biology (Grant DBI-0905806), and also while she was supported by the NSERC CREATE training pro- gram in biodiversity research; JR was supported by NCEAS, a Center funded by NSF (Grant EF-0553768), the University of California, Santa Barbara, and the State of California.
References
Alberti W, Anderson G, Bartolucci A et al. (1995) Chemotherapy in non-small-cell lung-cancer – a metaanalysis using updated data on individual patients from 52 randomized clinical-trials. British Medical Journal, 311, 899–909.
Baigent C, Keech A, Kearney PM et al. (2005) Efficacy and safety of cholesterol-lower- ing treatment: prospective meta-analysis of data from 90 056 participants in 14 randomised trials of statins. Lancet, 366, 1267–1278.
Barnes N (2010) Publish your computer publish your code: it is good enough. Nature, 467, 753.
Borer ET, Seabloom EW, Jones MB, M S (2009) Some simple guidelines for effective data management. Bulletin of the Ecological Society of America, 90, 205–214.
Carpenter SR, Armbrust EV, Arzberger PW et al. (2009) Accelerate synthesis in ecol- ogy and environmental sciences. BioScience, 59, 699–701.
Concato J, Shah N, Horwitz RI (2000) Randomized, controlled trials, observational studies, and the hierarchy of research designs. New England Journal of Medicine, 342, 1887–1892.
Cooper H, Hedges LV, Valentine JC (2009) The Handbook of Research Synthesis and Meta-Analysis (2nd edn). The Russell Sage Foundation, New York.
Davis CC, Willis CG, Primack RB, Miller-Rushing AJ (2010) The importance of phy- logeny to the study of phenological response to global climate change. Philosophical Transactions of the Royal Society B: Biological Sciences, 365, 3201–3213.
Devidas M, London WB, Anderson JR (2010) The use of central laboratories and remote electronic data capture to risk-adjust therapy for pediatric acute lympho- blastic leukemia and neuroblastoma. Seminars in Oncology, 37, 53–59.
Edwards M, Richardson AJ (2004) Impact of climate change on marine pelagic phe- nology and trophic mismatch. Nature, 430, 881–884.
© 2012 Blackwell Publishing Ltd, Global Change Biology, 18, 2102–2110
OPEN SCIENCE IN GLOBAL CHANGE RESEARCH 2109
2110 E. M. WOLKOVICH et al.
Edwards M, Beaugrand G, Hays GC, Koslow JA, Richardson AJ (2010) Multi-decadal oceanic ecological datasets and their application in marine policy and manage- ment. Trends in Ecology and Evolution, 25, 602–610.
Ellison AM (2010) Repeatability and transparency in ecological research. Ecology, 91, 2536–2539.
Fegraus EH, Andelman S, Jones MBMS (2005) Maximizing the value of ecological data with structured metadata: an introduction to ecological metadata language (eml) and principles for metadata creation. Bulletin of the Ecological Society of Amer- ica, 86, 158–168.
Field D, Sansone SA, Collis A et al. (2009) Megascience ‚omics data sharing. Science, 326, 234–236.
Fitter AH, Fitter RSR (2002) Rapid changes in flowering time in British plants. Science, 296, 1689–1691.
Goodchild MF (2000) Communicating the Results of Accuracy Assessment: Metadata, Digi- tal Libraries and Assessing Fitness for Use, pp. 3–16. Ann Arbor Press, Chelsea, MI. Gray ST, Betancourt JL, Jackson ST, Eddy RG (2006) Role of multidecadal climate var-
iability in a range extension of pinyon pine. Ecology, 87, 1124–1130.
Halpern BS, Walbridge S, Selkoe KA et al. (2008) A global map of human impact on
marine ecosystems. Science, 319, 948–952.
Hegerl G, Hoegh-Guldberg O, Casassa G et al. (2010) Meeting Report of the
Intergovernmental Panel on Climate Change Expert Meeting on Detection and Attribution of Anthropogenic Climate Change, chap. Good Practice Guidance Paper on Detection and Attribution Related to Anthropogenic Climate Change. IPCC Working Group I Technical Support Unit, University of Bern, Bern, Switzerland.
Holling CS (1978) Adaptive Environmental Assessment and Management, pp. 67–69. International Series on Applied Systems Analysys, John Wiley &amp; Sons, New York.
Hulme PE (2011) Contrasting impacts of climate-driven flowering phenology on changes in alien and native plant species distributions. New Phytologist, 189, 272– 281.
Igo-Kemenes P (2011) Keeping Data Alive for Long-term Re-use, pp. 14–15. Alliance for Permanent Access, The Hague, the Netherlands.
Ince DC, Hatton L, Graham-Cumming J (2012) The case for open computer programs. Nature, 482, 485–488.
Jackson ST, Overpeck JT, Webb T, Keattch SE, Anderson KH (1997) Mapped plant- macrofossil and pollen records of late quaternary vegetation change in eastern North America. Quaternary Science Reviews, 16, 1–70.
Kennedy D (2003) Multiple authors, multiple problems. Science, 301, 733.
Kolata G. Rare sharing of data leads to progress on Alzheimer’s. New York Times, 12
August 2010.
Kueffer C, Niinemets U, Drenovsky RE et al. (2011) Fame, glory and neglect in meta-
analyses. Trends in Ecology and Evolution, 26, 493–494.
Lee JW, Zhang JT, Zimmerman AS, Lucia A (2009) Datanet: an emerging cyberinfra-
structure for sharing, reusing and preserving digital data for scientific discovery
and learning. Aiche Journal, 55, 2757–2764.
Mervis J (2010) NSF to ask every grant applicant for data management plan. Science
Available at: http://news.sciencemag.org/scienceinsider/2010/05/nsf-to-ask-
every-grant-applicant.html (accessed 13 May 2010).
Mesirov JP (2010) Accessible reproducible research. Science, 327, 415.
Michener WK, Brunt JW, Helly JJ, Kirchner TB, Stafford SG (1997) Nongeospatial
metadata for the ecological sciences. Ecological Applications, 7, 330–342.
Nelson B (2009) Empty archives. Nature, 461, 160–163.
Overpeck JT, Meehl GA, Bony S, Easterling DR (2011) Climate data challenges in the
21st century. Science, 331, 700–702.
Parmesan C, Yohe G (2003) A globally coherent fingerprint of climate change impacts
across natural systems. Nature, 421, 37–42.
Parmesan C, Ryrholm N, Stefanescu C et al. (1999) Poleward shifts in geographical
ranges of butterfly species associated with regional warming. Nature, 399, 579–583. Parr CS, Cummings MP (2005) Data sharing in ecology and evolution. Trends in Ecol-
ogy and Evolution, 20, 362–363.
Piwowar HA, Day RS, Fridsma DB (2007) Sharing detailed research data is associated
with increased citation rate. PLoS ONE, 2, e308.
Piwowar HA, Becich MJ, Bilofsky H, Crowley RS; on behalf of the caBIG Data Shar- ing, Workspace IC (2008) Towards a data sharing culture: recommendations for leadership from academic health centers. PLoS Medicine, 5, e183.
Piwowar HA, Vision TJ, Whitlock MC (2011) Data archiving is a good investment. Nature, 473, 285.
Raff H (2003) A suggestion for the multiple author issue. Science, 302, 55–57. Raymond ES (1999) The Cathedral and the Bazaar. O’Reilly &amp; Associates, Inc., Sebasto-
pol, CA.
Reaman GH (2002) Pediatric oncology: current views and outcomes. Pediatric Clinics
of North America, 49, 1305.
Reichman OJ, Schildhauer MP, Jones MB (2011) Challenges and opportunities of open
data in ecology. Science, 331, 703–705.
Rezende EL, Lavabre JE, Guimaraes PR, Jordano P, Bascompte J (2007) Non-random
coextinctions in phylogenetically structured mutualistic networks. Nature, 448,
925–928.
Roberts L (2002) A tussle over the rules for DNA data sharing. Science, 298, 1312–
1313.
Root TL, Price JT, Hall KR, Schneider SH, Rosenzweig C, Pounds JA (2003) Finger-
prints of global warming on wild animals and plants. Nature, 421, 57–60.
Root TL, MacMynowski DP, Mastrandrea MD, Schneider SH (2005) Human-modified temperatures induce species changes: joint attribution. Proceedings of the National
Academy of Sciences of the United States of America, 102, 7465–7469.
Rosenzweig C, Karoly D, Vicarelli M et al. (2008) Attributing physical and biological
impacts to anthropogenic climate change. Nature, 453, 353–357.
Savage CJ, Vickers AJ (2009) Empirical study of data sharing by authors publishing
in PLoS journals. PLoS ONE, 4, e7078.
Schofield PN, Bubela T, Weaver T et al. (2009) Post-publication sharing of data and
tools. Nature, 461, 171–173.
Sih A, Bell AM, Kerby JL (2004) Two stressors are far deadlier than one. Trends in
Ecology and Evolution, 19, 274–276.
Sir Alister Hardy Foundation for Ocean Science (2012) CPR Bibliography. Available at:
http://www.sahfos.ac.uk/research/publications/cpr-bibliography.aspx (accessed
12 February 2012).
Smith VS (2009) Data publication: towards a database of everything. BMC Research
Notes, 2, 113.
Stein LD (2008) Wiki features and commenting – towards a cyberinfrastructure for
the biological sciences: progress, visions and challenges. Nature Reviews Genetics, 9,
678–688.
Stokstad E (2011) Network science: open-source ecology takes root across the world.
Science, 334, 308–309.
Strasser CRC, Michener W, Budden A, Koskela R (2011) Dataone: promoting data
stewardship through best practices.
Tenopir C, Allard S, Douglass K et al. (2011) Data sharing by scientists: practices and
perceptions. PLoS ONE, 6, e21101.
Unguru Y (2011) The successful integration of research and care: how pediatric oncol-
ogy became the subspecialty in which research defines the standard of care. Pediat-
ric Blood and Cancer, 56, 1019–1025.
Wall DH, Bradford MA, St John MG et al. (2008) Global decomposition experiment
shows soil animal impacts on decomposition are climate-dependent. Global Change
Biology, 14, 2661–2677.
Walther GR, Beissner S, Burga CA (2005) Trends in the upward shift of alpine plants.
Journal of Vegetation Science, 16, 541–548.
Weltzin JF, Belote RT, Williams LT, Keller JK, Engel EC (2006) Authorship in ecology:
attribution, accountability, and responsibility. Frontiers in Ecology and the Environ-
ment, 4, 435–441.
Whitlock MC (2011) Data archiving in ecology and evolution: best practices. Trends in
Ecology and Evolution, 26, 61–65.
Whitlock MC, McPeek MA, Rausher MD, Rieseberg L, Moore AJ (2010) Data archiv-
ing. The American Naturalist, 175, E45–E146.
Zimmerman AS (2008) New knowledge from old data – the role of standards in the
sharing and reuse of ecological data. Science Technology and Human Values, 33, 631– 652.
© 2012 Blackwell Publishing Ltd, Global Change Biology, 18, 2102–2110
</Text>
        </Document>
        <Document ID="3">
            <Title>My BARUG talk</Title>
            <Text>My BARUG talk

- Introduce myself
- My connection to R + open science
- What is Open Science?
- How R fits in?
  - Tools are free and can become instantly available to anyone anywhere.
  - CRAN pre-dates the app store and can easily disseminate code.
- What is lacking?
- R tools - introduce ropensci.org
  - RMendeley (citation processing).
  - DataONE
    - The possibility of pushing data into the cloud from R.
    - rProvenance
  - Ritis
  - rgbif
  - Dryad
  - Total Impact
  - Licenses -- we choose CC0, the first ever for an R package making these tools completely free.
- These are all use-cases for connecting R with widely available data.
  - Anything with an API
  - anything that works with oauth1 or oauth2
  - Mapping
    - CartoDB
- Encapsulate the entire research process in R.
- This includes report generation.
- Open CPU.org 
- Reproducible research and the executable paper
- Peer review process.


Created with WorkFlowy.com</Text>
        </Document>
        <Document ID="11">
            <Title>Research</Title>
            <Text>I will need to pull items from:

Lizzie’s Global change biology paper,
All the resources I already have on my Pinboard.
</Text>
        </Document>
        <Document ID="16">
            <Title>list of topics</Title>
            <Text>What is open science
	Data are re-used in ways not intended originally.  
Ropensci
	- Dryad, Mendeley, total-impact.
DataONE
	- Workflows, R package, provenance.
Writing with R
	Knitcitations.


(Use the webfont from symbolset too).


Team
The team consists of Carl Boettiger, Scott Chamberlain and me. We are all ecology postdocs. We also have a diverse advisory team with two R experts (Duncan temple lang and Hadley Wickham), Bertram Ludascher, an expert on scientfic workflow systems and provenance, and Matt Jones, who is the director of eco informatics at the national center for ecological analysis and synthesis. 


Provenance
An important feature of scientific workflow systems is their ability to record and subsequently query and visualize provenance information. Provenance includes the processing history and lineage of data, and can be used, e.g., to validate/invalidate outputs, debug workflows, document authorship and attribution chains, etc. and thus facilitate “reproducible science”. 

Trace files implemented as XML The results of each run of the Challenge workflow (including input data, intermediate and final data products, as well as provenance) were recorded in a trace file by the CollectionWriter actor and may be downloaded here: trace1.xml, trace2.xml. Trace files are implemented in XML using the same schema used for workflow input files read by CollectionReader.

What is provenance?
metadata needed to understand process that  created some result.
information that makes computation/data more "transparent", "trustworthy"
	Why is provenance 
important?
• long-term record keeping
• debugging, data cleaning, error diagnosis
• scientiﬁc repeatability
• data &amp; provenance required by some journals
• trust, accountability, transparency
• i.e., climategate-prevention

Git’s hyperdistributed peer-to-peer data sharing model is good for code, but bad for big scientific data. This is because Git works by placing a complete version of the entire code repository, including all versioning and branching, onto the laptop of every developer who forks (copies) the code base for their own personal use, and then allows them to pull (merge in) other people’s development as it proceeds.  This has turned out to be a very powerful way to develop because it gives enormous flexibility to developers to experiment, and then select the experiments that work.  This works because code is small and usually evolves though small and mostly orthogonal diffs (changes) in text files that are efficient to merge.   The rapidly increasing availability of sequencing technologies like RNAseq, full exome, and even full genome promises to increase the rate of data generation data faster than improvements in storage, and especially network bandwidth.  Furthermore, if you re-run an analysis, you’re probably going to change *all* the output results, and the analysis itself might require a warehouse full of computers to process in a reasonable time.  Giving everyone a copy of the data on their local machine just won’t work.  Giving analyst teams distributed access to shared and centralized data and compute resources is necessary, and becoming more technically straightforward given the rise of commercial cloud computing platforms.

R is great because it helps ideas spread faster.
: to slow down the competition by keeping the results of hard work to yourself. Daniel Lemire, a computer scientist and professor, responded to this argument elsewhere by pointing out that open sourcing his code not only makes his work repeatable, but spreads the ideas faster and makes the code better in the long run, since other users can help debug it.

Markdown is becoming an increasingly popular platform for lightweight and online publishing.

Datacite works increase acceptance of research data as legitimate, citable contributions to the scholarly record

Benefits of open science

Data gets cited more. 
New ideas are discovered from data that’s already been paid for.</Text>
        </Document>
        <Document ID="4">
            <Title>Introduction</Title>
        </Document>
        <Document ID="5">
            <Title>What is open science?</Title>
        </Document>
        <Document ID="12">
            <Title>Full outline</Title>
            <Text>Introduce myself.
Next, talk about why I got involved iwth open science.
Then how and why I use R
What is Open Science?
Data and literature should be open
People who collect data from publicly funded research are obligated to make these available in the future, especially once they have acheived their objective.

R is great because it is free, the tools can be developed by anyone. And R does a great job talking to APIs. RESTful interface.
R does great visualization tools. - Ggplot and for mapping purposes there is cartodb.

Slide 1:
Thanks for having me. My name’s Karthik Ram and I am a postdoctoral fellow in Ecology at Cal. I’ve been using R since 2003 on and off and was constantly discouraged from using it in favor of SAS since it was more stable, commercially supported and licenses were inexpensive. But as I started working through, there was little help to be sought, I couldn’t afford separate licenses for every computer that I was on. I was lucky enough to teach for a professor who gave his students the option of using R. Even though the learning curve was steep, all but one student began using R. That’s when I realized the sparse, austere?, command-line interface, which has always turned away non-techy academics could potentially transform the academic landscape.


But since I am talking to you today about R in the context of Open Science, I’ll start with what that means. Open Science is the idea adopted by a growing number of academics that the outputs of science, and products developed along the way should not be locked away behind paywalls. At the core, the movement supports open access journals, sharing results and data both pre and post publication. 

Started with PLOS, now every journal has an open access policy. At it’s core, publishing in open access gets cited more. R fits in perfectly with this philosopy with the software being both open source, but more importantly, having a large user-base that can contribute code without too much difficulty.
SHOW # OF R PACKAGES GROWING ON CRAN.

Another big, but related problem is:
A) reproducibility. Most research going from lab notebooks to spreadsheets, then gets transformed, corrected before final analysis. These first few steps are lost in the workflow. But now R, with it’s powerful and easy tools, makes it easy to keep raw data but transform them for analysis along the way.

</Text>
        </Document>
        <Document ID="17">
            <Title>oauth</Title>
            <Text>￼￼￼Chapter 1. Authentication with Web Services via OAuth
Table of Contents
1. Securely Accessing Private Data with OAuth ............................................................................... 2 1.1. The OAuth Model and R ................................................................................................. 3 1.2. Creating/Registering an Application with the Provider ....................................................... 4
2. The ROAuth package ................................................................................................................ 4 2.1. The Basic Workflow in R for OAuth 1.0 .......................................................................... 5 2.1.1. Creating the Application OAuthCredentials Object .......................................... 5 2.1.2. The Handshake to Get the User Permission and Access Token ................................. 6 2.1.3. Invoking Methods with the OAuth access token - OAuthRequest() ....................... 9 2.2. Using an Access Token Across R Sessions ...................................................................... 10 2.3. Keeping the Consumer Key and Secret Private ................................................................ 10 2.4. Extending the OAuthCredentials Class .................................................................... 11 2.5. An Alternative Syntax for Invoking OAuth Requests ........................................................ 12 2.6. Low-level Details of OAuth 1.0 ...................................................................................... 12 2.6.1. The OAuth 1.0 Handshake ................................................................................... 12 2.6.2. The OAuth 1.0 Digital Signature Details ............................................................... 13 3. OAuth 2.0 ................................................................................................................................ 14 3.1. Google Storage .............................................................................................................. 14 3.1.1. Getting the User's Permission and the Authorization Token .................................... 15 3.1.2. Exchanging the Authorization Token for an Access Token ..................................... 17 3.1.3. Using the Access Token in an API Request .......................................................... 18 3.1.4. Refreshing an OAuth2 Access Token ................................................................... 20 4. Further Reading ........................................................................................................................ 21
￼￼￼￼￼Summary
Some REST APIs require authentication and some require us to use a more general mech- anism named OAuth to avoid logins and passwords and allow three-party secure interac- tions. We'll describe this mechanism and how to use the ROAuth package in R to work with REST APIs requiring authentication with OAuth 1.0, We'll also illustrate how to work with OAuth 2.0 in R. We'll illustrate these using Dropbox and Google Storage as examples.
￼￼Questions.
1. get to R quicker?
2. get to Dropbox as the example quicker?
3. move the caveats and notes about how handshaking to use OAuth in R to end of section of the ROAuth package
￼￼￼￼1 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼1. Securely Accessing Private Data with
OAuth
When we use the Web we are using a client-server model where our browser or R (or another application) is the client and the Web server is the provider of the resource, e.g. an HTML document, a CGI script. In this case, the client is anonymous. The server knows the IP address of the client, but not the identity of the person making the request. For public sites and pages, this is fine as the server doesn't need to know who is requesting the page/resource in order to return the information. Of course, when the page or services con- tains data that is restricted to a specific person or group of people, the client cannot access it anonymously. Instead, the client must identify the user correctly, providing the appropriate credentials. For this, we typi- cally use a login and password. We can specify these in an HTTP request using RCurl via the userpwd curl option of the form "login:password", or the username and password pair of options if we want to separate the login and password.
This simple two-way or two-legged authentication is quite simple and familiar. Increasingly, however, we encounter more complex and richer situations which involves three parties in the interaction. Suppose we have a user who wants to allow an application on her desktop or a Web site to access data/resources on a different Web site. The OAuth documentation hueniverse_ProtocolWorkflow describes the following nice example. Suppose we have photos on a Web site such as Flickr. We want to use an online printing service - PhotoPrint - to order physical copies of the photos. While we can download the photos from Flickr and then upload them to the printing site, PhotoPrint provides a neat feature that presents us with a list of our photos and allows us to select which ones we want to print. But how does the photo service get a list of our photos? This is private information and should require us to login to Flickr. In this case, we have three parties in the interaction. Flickr is the provider of the resources - the photos. We are the user who owns the resources (but we are not the provider). PhotoPrint is the application that needs access to these resources from the provider and must be given permission by us - the user and owner - to access them.
Under no circumstances do we want to give the photo service our login and password for Flickr. But we do want to authorize them to get a list of the photos, and also to be able to access the photos we select. We don't want to allow them to delete any photos, change our account in any way, etc. So we want to give them limited privileges and for a short period of time. We also want to be able to revoke these privileges or have them naturally expire. Instead of giving them our login and password, the application, user/owner and provider communicate and give the application a token, or a ticket, that grants them explicit and specific privileges. The application then uses this token in each request it makes to the provider's API so that it is allowed access the user's resources. The OAuth mechanism allows us to do all this. The OAuth guide on hueniverse.com describes OAuth in the following way: “ OAuth provides a method for users to grant third-party access to their resources without sharing their passwords. It also provides a way to grant limited access (in scope, duration, etc.). ”
OAuth is much more general, powerful and complex than the simple login-password used for client-server authentication. OAuth overcomes lots of the insecurities and limitations in basic authentication using a login and password. However, it is more complex. Not only do we have to coordinate all three parties involved to grant and gain the relevant permissions, the OAuth 1.0 mechanism requires that we sign each request to access the privileged data. This signature mechanism is quite complex. The ROAuth package hides these details. OAuth2 - the next generation version of the authentication mechanism - is a great deal simpler and more direct. It uses secure HTTP (HTTPS) to remove the complex signature mechanism. While OAuth2 is not a finalized specification, it is in use by significant providers such as Google, Facebook (for their Graph API), LinkedIn and will be used increasingly in the future.
￼￼￼￼Securely Accessing Private Data with OAuth
2 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼Our focus on OAuth in this book is using it from within R. Specifically, we are looking at invoking methods offered by a provider's API so that we can access private resources belong to a user from within R. We use OAuth to negotiate between R, the user and the provider to gain an access token. We then use OAuth and this access token to invoke methods using an HTTP request. We us the ROAuth package, rather than RCurl directly, to make these HTTP requests in order to hide all of the extra details OAuth requires.
In this chapter, we start by looking at OAuth 1.0 and the ROAuth package. We'll use Dropbox's API to be able to download and upload files from and to our Dropbox account from within R. We illustrate OAuth 2.0 by accessing the Google Storage API. We should note that in both cases, there are packages that provide high-level R functions that interface to the methods of these two APIs - rDrop and RGoogleStorage. These functions hide much of the details of how we use OAuth in R. However, we illustrate some of these in this chapter. The ROAuth package itself hides most of the details of using OAuth (e.g. signing the contents of requests) In this chapter, we won't focus on those low-level details, but instead look at how to use OAuth in R via the ROAuth package. You can read a great deal more about the lower-level details of OAuth in the OAuth specification ???, on the Web ???, and in the book ???
1.1. The OAuth Model and R
Before we discuss the ROAuth package and specific R functions, it is important to understand how we think about the OAuth model in the context of using it from R to call methods in a provider's API. The general OAuth mechanism is for situations where there are three participants. There is the resource owner/user (i.e. us) who has to authorize an application to have access to the resources maintained by a provider/server. The user, application and provider are the three parties. When we are working within R, the provider is the remote Web service whose methods we want to invoke. In most cases, the owner will be the R user and the application will also be the combination of R and the owner. That an user/owner will have her own application. So while there are three parties, two of them (the user and application) are very tightly coupled. As a result, the regular three-participant OAuth approach is a lot more complicated than it need be for this situation. Indeed, a login and password would be much more convenient for us. However, since OAuth is much more flexible and also widely used in other situations, we often need to use it. Accordingly, the common approach to using OAuth in R is that each owner will create her own application. (We'll see how to do this for Dropbox below. ) To connect all of this to OAuth's three-legged model you can think of yourself (the user) as being the resource owner, R as being the application and the provider, e.g. Dropbox, as being the server, or host, of the resources.
When a user creates an application, she registers it with the provider (typically via a Web page). She specifies a name and a description of the app. The application is much more than a name and description. When we register the application, the provider gives two tokens - a consumer key and secret. These uniquely identify the application.
Why should each owner have to create her own application? Instead, we might be tempted to use OAuth in the more common 3-legged manner. In the case of Dropbox, for example, we could create an R package (say rDrop) that provides an interface to the Dropbox API and which acts as a third-party separate application. The developer of the rDrop package would register it with Dropbox. It would have its own consumer key and secret. We could put these in the code for the package and then use them to obtain the access token by having each user grant access to their own data. This appears to make a lot more sense than having each user create her own consumer key and secret for the same R package. However, it is a very bad idea. Why? Consider the case where we do have a key and secret for the rDrop package. These credentials must be
￼￼￼￼￼￼￼￼￼￼￼￼The OAuth Model and R
3 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼in the R code for the package so that it could use them to perform the handshake()1. This means that other programmers could easily obtain these private key and secret and use them in their own nefarious packages. When they ask a user to grant permission for their code, they could use the key and secret from other code outside of the rDrop package. Users would unknowingly grant access to code that they thought was from the rDroppackage, and the other package would then be able to do what it wanted with the access privileges, and the user would blame our package. We could use an application-specific consumer key and secret if we were running R code on a server and so nobody saw that code. However, once we distribute the R package to run on other systems not under our control, the consumer key and secret are no longer secure.
One might consider putting the consumer key and secret in compiled code. Again, this is a problem if you give the source code for the package to others to install as an R package. If you just provide binaries, people can still use a program such as strings to extract those values. Furthermore, if the R code in the package can query the key and secret from the native code and then use them within R code, then an R user can step through R code and discover this private data directly. To avoid this, the compiled code would have to do all the signing of the HTTP requests directly itself. There are libraries to help with this (liboauth and an R package that uses it), but this is undesirable for several reasons.
1.2. Creating/Registering an Application with the Provider
Before we can access a provider's API in R, we need to register an application with the provider. This will give us the consumer key and secret that uniquely identifies the application to the provider. How we register the application differs slightly for each provider, but the basic steps are the same. We login to the provider's Web page using our account for that site, find the relevant Web page for registering developer applications and fill in a form. Assuming there are no errors, the provider will show you the consumer key and secret pair for the new application. You must store these and ensure that they are private, i.e. nobody else can read them.
In order to follow along with our example, you can register an application for Dropbox. We visit the page https://www.dropbox.com/developers/apps and click on the button in the middle of the page entitled "Create an App". Note that this page also lists the existing applications we have registered and we can find their information, including the key and secret, should we have lost them. We can also delete them and so revoke access for those applications. We specify a name for the application. Almost any name is fine. You might use the login name for the Dropbox account prefixed by the letter "R". We'll use "RJaneDoe" for this chapter. You also provide a brief description. It is important to change the default "Access level" option to "Full Dropbox". This is how we control the scope of the privileges. We click on the "Create" button and the Dropbox site shows us a new page with the key and secret for our new application. We will need to record those. It is important to keep these private.
2. The ROAuth package
Rather than describing the OAuth mechanism and its low-level details, we'll start by focusing on how we use it in R via the ROAuth package. After we see it in practice, we'll give a high-level overview of two aspects of how OAuth works (negotiating the access token and signing the HTTP requests) in the hope that this demystifies the steps and helps when things go awry.
1There is no point in locating them at a URL as they are still accessible to others. Similarly, keeping them in the package but not in the R code still makes them accessible. Basically, anytime we have an open source package, it cannot restrict access to the keys. Instead, we must have each user provide the key and secret and that is what we are doing when they register their own application and get their own pair of key and secret.
￼￼￼￼￼￼￼￼￼Creating/Registering an Application with the Provider
4 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼2.1. The Basic Workflow in R for OAuth 1.0
There are three pieces to the computational model for R that you need to understand to make use of ROAuth. The first is that we use the oauth() function to create an R object that contains the application's consumer key and secret, and information about how to communicate with the provider. The key and secret are for our "application", and are associated with our account on the provider, e.g. Dropbox. The second step is to use the consumer key and secret to get an access token for making actual HTTP requests to the provider's API. The handshake() function does the negotiating, involving the user to grant the permissions via a Web page on the provider's site. The final piece is that each request needs to use the credentials object to sign the content of the request. This will happen automatically by using OAuthRequest() function for each call to a method in the provider's API.
The basic sequence of operations in pseudo-code are
We first combine the application information for the OAuth provider. Then we negotiate between the ap- plication, provider and user to get an access token. Then we use this access token to invoke methods in the provider's API.
2.1.1. Creating the Application OAuthCredentials Object
The oauth() function assembles the necessary details in order to be able to obtain an access token from the provider. The oauth() function typically requires five arguments. The first two are the application's consumer key and secret that were created by the provider when we registered our application. The remain- ing three arguments are URLs that tell the oauth() function how to negotiate for an access token. These are the request URL, authorize URL and access URL. We'll see later that OAuth uses these in sequence to obtain the final access token that has been verified by the user, i.e. owner of the resources we are going to access. For Dropbox, these URLs are listed and explained on the API reference page. Each OAuth provider will provide documentation the tells us its URLs for these tasks.
We can now call the oauth() function as
￼￼￼￼￼cred = oauth(key, secret, requestURL, authURL, accessURL)
cred = handshake(cred)
OAuthRequest(cred, methodURL,
             listOfArgs,
             method = "GET",
             curl_option, curl_option,
             curl = curlHandle)
￼￼￼￼￼cred = oauth(consumerKey = cKey, consumerSecret = cSecret,
            requestURL = "https://api.dropbox.com/1/oauth/request_token
            authURL = "https://www.dropbox.com/1/oauth/authorize"
            accessURL = "https://api.dropbox.com/1/oauth/access_token/"
            post = FALSE)
cKey
cSecret
", ,
We assume here that the consumer key and secret are stored in the variables ￼ and ￼ . This is a good practice as they are secret and should never be visible in code. (See below for ideas about how to store them.)
oauth() just creates an R object of class OAuthCredentials. It does not communicate with the provider - Dropbox - at this point.
￼￼￼￼The Basic Workflow in R for OAuth 1.0
5 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼Move or remove
The negotiations to get the token and secret necessary for making the actual requests on behalf of the owner involve several steps. They also involve additional security that involves signing the contents of the request in such a way that the receiver can verify that they came from the correct source and were not intercepted and changed, or being re-submitted (e.g. transferring money for a second time). This signature process also involves several steps and some less familiar technologies, e.g. digital signatures. The ROAuth package hides these complexities from you, and allows you to focus on invoking the Web service methods.
2.1.2. The Handshake to Get the User Permission and Access Token
Now that we have specified all the details, we can start the negotiation for authorization and the relevant tokens. For Dropbox, we do this by calling the handshake method:
(We can also use the form
cred = cred$handshake(post = FALSE, verify = "...")
) After we call this function, there is a slight delay and then we see the message that asks us to authorize the access using our message in the verify argument. Then, our Web browser comes to the foreground and we are displayed a page asking us to grant permission to the application we registered with the provider, Dropbox. If we are not already logged into our account on Dropbox in our Web browser, the browser will direct us to the login page and after entering our login and password, the actual page with which we grant permission to our application will appear. It looks something like
￼￼cred = handshake(cred, post = FALSE,
                 verify = "hit return in R when you authorize access"))
￼￼￼￼￼The Basic Workflow in R for OAuth 1.0
6 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼￼￼￼Figure 1.1.
￼￼We then grant permission by clicking the "Allow" button and we see the next page
￼￼￼The Basic Workflow in R for OAuth 1.0
7 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼￼￼￼Figure 1.2.
￼￼For Dropbox, we return to the R prompt and hit enter so that the handshake operation can proceed.
We should note that most OAuth providers will issue a new token after you grant permissions to the appli- cation. This will be shown on a Web page when you grant access. You then copy that from the browser back into R as the oauth() function needs this token before it can proceed to the next stage to get the final access token. Dropbox is unusual in that it doesn't issue an new token. This is why we use the verify parameter to specify a message to prompt the user to hit the return key after they granted permission to the application. Generally, oauth() will prompt the user with something like
This tells the user which URL to visit in case she has to visit this manually and also to copy the new token back to R.
￼￼￼￼To enable the connection, please direct your web browser to:
https://www.dropbox.com/1/oauth/authorize?oauth_token=xxxxxxxxxx
When complete, record the PIN given to you and provide it here,
or hit enter:
￼￼￼The Basic Workflow in R for OAuth 1.0
8 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼It is important to note that the handshake() function returns the updated OAuthCredentials object with the additional key and secret that allows us to make requests. It is essential that you assign the object to an R variable to be able to use it in future calls. The OAuthCredentials object you created with the call to oauth() does not have these key and secret, just your own consumer key and secret. So if you do not assign the updated version, it will be as if you never went through the handshake process.
The post argument in the call to handshake() controls how we perform the handshaking HTTP requests, either using GET or POST operations. POST is the default approach as many OAuth servers use POST and this is the recommended approach. We had to override it in the case of Dropbox.
As with all of the OAuth calls we are making, we are using an HTTP request and we can customize this with Curl options. You can specify these in any OAuth call (handshake() orOAuthRequest()) as individual arguments or as a list of options via the .opts parameter. We can also pass a Curl handle to use for the request via the curl parameter.
2.1.3. Invoking Methods with the OAuth access token - OAu- thRequest()
Once we have completed the handshake to acquire the authentication tokens, we can use it repeatedly to call privileged methods in the provider's API. We do this by calling the OAuthRequest() function. The first argument is the OAuthCredential object (with the newly updated access key and secret). The second is the URL for the request. Next, we pass a list or character vector of the arguments for the call. We can also specify the type or method of the HTTP request, i.e. GET, POST, PUT, DELETE, etc. The default is GET. For example, the Dropbox API has a method for getting meta-information about the user's account. (See https://api.dropbox.com/developers/reference/api.) This method takes no inputs and is a regular "GET" request. So we can invoke it with
val = OAuthRequest(cred, "https://api.dropbox.com/1/account/info")
The result is a JSON string and we can convert this to an R object with fromJSON(val)
Again, we can specify Curl options or a Curl handle to control the HTTP request. One can pass options to control how the HTTP request is performed via the ... parameter of OAuthRequest(). For example, we can specify the user-agent for the request and also instruct the request to follow redirects with
These arguments are passed on to , ￼ and similarly named functions, selected based on the value of the argument.
In most cases we'll pass actual arguments to parameterize the method. For example, if we want to create a new folder in Dropbox, we have to specify the root folder (either "dropbox" or "sandbox") and the path or name of the new folder. This is a POST request rather than a GET operation. So we can invoke this as
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼val = OAuthRequest(cred, "https://api.dropbox.com/1/account/info",
                    httpheader = c('User-agent' = 'rDrop'),
                    followlocation = TRUE, verbose = TRUE)
oauthGET()
oauthPOST()
￼method
￼￼val = OAuthRequest(cred,
                   "https://api.dropbox.com/1/fileops/create_folder",
                   c(root = "dropbox", path = "NewFolder"),
                   method = "POST",
                   httpheader = c('User-agent' = 'rDrop'),
                   followlocation = TRUE, verbose = TRUE)
￼￼￼The Basic Workflow in R for OAuth 1.0
9 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼Again we specify Curl options, but the interesting argument to our function is the collection of individual arguments passed to the REST method. These are root and path. Here they are passed as a vector, but this can also be a list.
We've mentioned that OAuth 1.0 uses a regular HTTP request, but involves a somewhat complex process of signing the requests and information to the header in the HTTP request. The OAuthRequest() function doesn't require input from us to deal with this and uses the information in the OAuthCredentials object to take care of all of the details on our behalf. For the curious, we'll discuss the details later in this chapter.
There are some sites that do not require the user-authorization step in the OAuth negotiations. They use OAuth so that they can reliably identify the application making the requests, or simply when there is no specific user but general access. They may want to do this for auditing purposes or to provide different services for different applications, e.g. provide faster response for premium applications. In these cases, we can simply omit the authentication URL when creating the OAuthCredentials object, or directly in the call to handshake(). For example, the site http://term.ie/oauth/example provides a test site for OAuth1. It does not require user authentication as there is no user. It uses the literal strings "key" and "secret" as the consumer key and secret for all applications. We can obtain our access token using the call
hp",
")
￼￼￼cred = oauth("key", "secret",
             requestURL = "http://term.ie/oauth/example/request_token.p
             accessURL = "http://term.ie/oauth/example/access_token.php
authURL
handshake()
We simply omit the ￼ in the call. Then we can call ￼ and this performs the two-step request to gain the access token.
2.2. Using an Access Token Across R Sessions
¿¿¿¿¿
[Done] I moved so that it is beside Keeping the Consumer Key and Secret Private. This, that and Extending the OAuthCredentials Class and An Alternative Syntax for Invoking OAuth Requests could go in their own section as separate from the mechanics of OAuth 1.0 in R. Done
????
At the end of a successful call to handshake(), the cred object will have the information to make autho- rized requests to access the protected resources. It would be nice to only have to do this handshake and grant permissions for a particular application a single time. We can achieve this by saving the cred variable in R's usual way, i.e. via the save() function. Then we can load it into other R sessions and the tokens will still be valid (unless the user has revoked the permissions on the server). Of course, one can also call the handshake() function again and repeat the negotiations and authorization. Remember that the contents of this object are secret and so it is important to be careful that nobody else can read the saved file. If somebody does access it, they can call any of the provider's API methods.
2.3. Keeping the Consumer Key and Secret Private
We have to provide our application's key and secret in the call to oauth(). It is important that these remain private and are not shared with other people. As a result, they should not appear in code that can be read by another person. There are several approaches to help with this. One is to set these values as an R option, typically in our .Rprofile file that is read when R starts. (It is important to ensure that this file is not readable by anybody else on your computer and also that you don't display your options in an R session.) Our code can refer to those options and then the values will never be seen in the code. For example, we can call oauth() with
￼￼￼￼￼￼￼￼￼￼Using an Access Token Across R Sessions
10 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼￼oauth(getOption("DropboxKey"), getOption("DropboxSecret"),
        requestURL, accessURL, authURL)
oauth()
Similarly, we could also assign the values to variables and then refer to those in our call ￼ as we did above
2.4. Extending the OAuthCredentials Class
It is convenient to combine and store the consumer key and secret along with the URLs needed for the OAuth negotiations to gain an access token. By combining them in a OAuthCredentials object, we can easily pass all of these values to handshake(). Of course, it is just as easy to pass them to handshake() individually, e.g.
and this also works. We can also use the resulting OAuthCredentials object ￼ in another call to handshake() to obtain a new access token.
One of the benefits of using the OAuthCredentials object in the first place is that we can define simple sub-classes of it for different services. For example, we might define one for Dropbox and another for Mendeley using
The purpose of these classes is that we can create more specific OAuthCredential objects and pass them to R functions that are wrappers for the Web service methods for that particular service. These functions need only check that the credentials are of the appropriate class rather than checking the URL string for the request. For example, we might implement downloading a file from Dropbox as
￼￼￼cred = handshake(c(consumerKey, consumerSecret),
                 "https://api.dropbox.com/1/oauth/request_token",
                 "https://www.dropbox.com/1/oauth/authorize",
                 "https://api.dropbox.com/1/oauth/access_token/")
cred
￼￼setClass("DropboxCredentials", contains = "OAuthCredentials")
setClass("MendeleyCredentials", contains = "OAuthCredentials")
￼dropbox_get =
function(cred, filename, binary = NA, ..., curl = getCurlHandle())
{
   if(!is(cred, "DropboxCredentials"))
     stop("invalid credentials")
   OAuthRequest(cred, "https://api-content.dropbox.com/1/files/",
                c(root = "dropbox",  path = filename), binary = binary,
                ..., curl = curl)
}
Using a sub-class is good practice as it catches mistakes of using credentials for one service with another and getting quite confused. The way we have written this wrapper function above (dropbox_get()) also illustrates several related good practices. The function takes the credential object and then explicit arguments for the parameters expected by the REST request. Importantly, it also takes a Curl handle via the curl parameter and any number of other arguments (via the ... parameter). It uses these in each HTTP request within the function. This allows the caller to control the HTTP request and also reuse an existing Curl handle.
￼￼￼￼￼Extending the OAuthCredentials Class
11 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼2.5. An Alternative Syntax for Invoking OAuth Re- quests
It can be convenient to think of the request as being a part of the OAuthCredentials object. We can call the OAuthRequest() function as
This helps to avoid the situation where we forget to pass the object as the first argument, but instead just focus on the parameters for the specific method we are calling.
The ￼ function looks at the method and determines which helper function to invoke, e.g. ￼ or . We can by-pass this by using the invocation form
cred$get(url, params, ...)
where we substitute the "get" with any of post, put, delete, head to specify the method.
Note that this form of invocation - cred$OAuthRequest() - is also compatible with the original ref- erence-class based OAuth class in the package. That particular implementation can cause problems when the credential objects are serialized and reused after the ROAuth package has been changed and improved. Instead of using the new package's methods, the older object uses the older methods from the earlier version of the package. That approach is still available in the package but, for various reasons, we encourage people to use the S4 class OAuthCredentials and the handshake() function for OAuth 1.0.
2.6. Low-level Details of OAuth 1.0
In this section, we'll give a brief description of two important aspects of OAuth. The first is how we use the application's consumer key and secret to get an access token that we can use to make actual HTTP requests to access the protected data. The second is how we digitally sign the HTTP requests to ensure their integrity and validity.
2.6.1. The OAuth 1.0 Handshake
We use the handshake() function to allow the application to obtain the important access token to access the user's resources on the provider. It can then use this to call methods in the provider's API to access the user's resources. We don't need to understand how the handshake() function gets the access token to use it. However, we explain the details next for those who are interested or need to know.
Generally in a call to oauth(), we have to specify the request URL, the authentication URL and also the access URL for OAuth. However, we only seem to visit one page in the Web browser. It is reasonable to ask what is the purpose of these URLs? The client uses the request, authorization and access URLs in three sequential steps. In each of these, the purpose is to get a token which it can use in the next step. At the end, the application has a token it can then use to access the resources owned by the user and made available via the provider. The user is us, say JaneDoe. The application is RJaneDoe that we registered with the provider, Dropbox. When we registered the application, we were given a key and secret for the application and this is the key-secret pair we pass to oauth(). For accessing Dropbox, we would create the OAuthCredentials with a call such as
￼￼cred$OAuthRequest(url, params, methods, ...)
cred
￼OAuthRequest()
￼oauthGET()
oauthDELETE()
￼￼￼￼￼￼￼￼￼￼cred =
  oauth(consumerKey = cKey, consumerSecret = cSecret,
        requestURL =   "https://api.dropbox.com/1/oauth/request_token",
￼￼￼An Alternative Syntax for Invoking OAuth Requests
12 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼￼authURL =  "https://www.dropbox.com/1/oauth/authorize",
accessURL = "https://api.dropbox.com/1/oauth/access_token/")
If it helps, we can think of the OAuthCredentials object as the application in the OAuth workflow. We, the R user, are the user in the OAuth workflow. Of course, Dropbox is the provider, and the files in our Dropbox account are the resources.
Step 1 - application asks provider for an unverified request token. When we call the handshake() function with the value of the cred variable, the application (our cred object) contacts the provider (Drop- box) using the request URL to ask the provider for a request token. This is a token which the application can use to ask the user for permission to access the resources on the provider.
The application asks the provider for the request token using its consumer key and secret. The provider checks these are valid application identifiers and issues the token. If the key or secret is not correct, the provider doesn't issue a token and application won't be able to move to the next step.
Step 2 - user grants authorization to the application. The application takes the unverified request token and asks the user to authorize it via the provider's Web site, specifically the authorization URL (given by the authURL argument in oauth()). This requires the user to be logged into the provider's Web site and so this might first bring the user to the provider's login page and then to the actual authorization URL. When the user grants the permissions requested by the application, the provider issues a verified request token. This is the string that the user copies from the browser back to the R session where the oauth() function is waiting before it can proceed to the third and final step.
Dropbox, in fact, does not issue a new token for the verified request token. In this case, we can skip the step of copying the new verified request token to R, since there is no new token.
Step 3 - application exchanges the verified request token for an access token. The final step in the handshake involves the application exchanging the user-verified request token for an access token that it can use to actually access the user's resources on the provider. It sends the user-verified request token (along with the application's own key and secret) to the provider via the access URL (given by the accessURL parameter). The provider returns a new access token. The application then stores that. The application can then use this access token in any later calls to methods in the provider's API in order to access the user's resources.
As we mentioned, the entire handshake process focuses on tokens and there are three tokens - the (unverified) request token, the verified request token and the access token. The user is involved in the second step; only the application and provider are involved in the negotiations for the first and third step.
2.6.2. The OAuth 1.0 Digital Signature Details
The ROAuth package also digitally signs each HTTP request performed via OAuthRequest() and its helper functions (oauthGET(), oauthPOST(), etc.). This digital signature involves combining the URL of the method request, all of the parameters in the request, the access token, the consumer key, a time stamp to identify to the server when the request was constructed (and avoid people replaying them at a later date). These are combined into a single string and then the "signature" or hash-based authentication code (HMAC) (or other signing methods) for that string is computed using the consumer secret. This string is then attached to the request in the HTTP header via an Authorization field so that the server can verify the contents of the request and ensure that they haven't been modified by anybody else. Show an actual request and the headers that it contains
￼￼￼￼￼￼￼￼￼￼￼￼￼￼Low-level Details of OAuth 1.0
13 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼&lt;duncan>If you want to do this OK with me, but I don't think it's essential&lt;/duncan>
The server verifies the signature by looking up its own copy of the secrets associated with the consumer key and the access token. It then has all of the information that the signer had and so can reproduce the signature. If they do not match, the server rejects the request. The signatures may not match because of an error in the client's signing code (or indeed the server too). However, when the code is correct, any mismatch indicates that somebody changed the actual request, e.g. one or more of the parameters. Others cannot create an appropriate new signature because they do not have the consumer and access secret. The server also checks the time stamp and will reject a request if it is from too long ago. It has to allow for some network latency delay in the request, but it will reject a request if it exceeds some threshold. The signature also uses a "nonce", a random number whose sole purpose is to be used just once. This nonce is contained in the signature and in the Authorization field. The server examines the nonce and looks at its records. If the nonce was used before, the request is rejected as being invalid. This prohibits a malicious man-in-the- middle from intercepting the request and replaying it another time, e.g. transferring money from an account.
There are many more details in the signing process which we have not discussed here, such as (lexico- graphically) ordering the parameters in the entire request, escaping certain characters by converting them to base-64, etc.
3. OAuth 2.0
Up to now, we have described OAuth 1.0. As we mentioned, it is powerful but the details are com- plex, specifically the digital signing of each request. Fortunately, ROAuth package hides these. However, OAuth2 is a good deal simpler as it uses secure HTTP (HTTPS) and avoids the need for a complicated sig- nature process. Because it is quite simple, we can manually and explicitly implement the necessary OAuth2 steps to both obtain a token and to use the token to make a request. We'll show how to do this using Google Storage as a specific example. You can read the code to get the idea. However, to run the code you will need to register an application with Google Storage API. Before that, you will need to enable the Google Storage service for your Google login. See for instructions on how to do this.
3.1. Google Storage
Google Storage is a RESTful Web service that allows people to store, access and share data in the cloud in a secure manner, similar in spirit to Amazon's Simple Storage Service (S3) [???]. We can upload, download and copy files in buckets (corresponding to folders) and query, set and change permissions (access control lists) using HTTPS requests. Authentication is done using OAuth 2.0. See https://developers.google.com/ storage/ for more details about setting up an account, using the API, etc. The RGoogleStorage package provides an R interface to this API and hides much of the OAuth 2.0 details. However, in this section, we'll look at how the package performs the authentication to illustrate how we can work with OAuth 2.0 in R for other providers.
The workflow for using OAuth 2.0 has a basic similarity to that of OAuth 1.0. We still have the application, user/owner and provider. In this case, Google Storage API is the provider. We, again, have to register an application with the provider. As with OAuth 1.0, this results in a private key and secret so must be done for each user wanting to access Google Storage from within R. We create the application via Google's API console[???https://code.google.com/apis/console#access]. The steps involve creating a new project, selecting the API or service of interest, specifying a product name, setting the "Application type" option to "Installed application" (rather than a Web application or Service account) and generating an OAuth 2.0
￼￼￼￼OAuth 2.0
14 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼client id. We won't describe specifically how to do all of this as there is up-to-date documentation on the API Web page. The successful creation of an application rewill be a Web page that looks like the page shown in Figure 1.3 (page 15).
The important information we will need are the Client ID and the Client secret. We assign these to an R variable or set them as options() in R so we can refer to them without showing them in the code.
3.1.1. Getting the User's Permission and the Authorization To- ken
We now have our application set up. Next, we need to have the user (also us) authorize access to our Google Storage account and its buckets and files. As with OAuth 1.0, this is a negotiation between all three parties - the user, the application and the provider. However, it is a little simpler than with OAuth 1.0. We'll leverage the Web browser to have the user visit a Web page to grant our application the authorization it needs.
￼￼￼Figure 1.3. Creating an application for accessing Google Storage
Having created the application to access Google Storage, we need the values of the Client ID and Client secret in R in order to get authorization and access tokens.
￼￼￼￼￼￼Google Storage
15 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼To do this, we construct the URL to visit in R and tell the user's Web browser to view that page via the browseURL() function in R. We cannot do this with an RCurl request as we need the Web browser so that the user can login to their Google account and to interact with the page to grant authorization and obtain the authorization token.
To create the URL, we have to combine our application identifier (Client ID from above) and also informa- tion indicating the scope, or set of permissions, we want access for, e.g. read, read and write, or just write. We send the request to https://accounts.google.com/o/oauth2/auth. Note that we are using HTTPS in all of our OAuth 2.0 requests. We add on the different parameters as part of a GET request, i.e. as name=value pairs, separated from each other via '&amp;' and separated from the URL by '?'. So the URL might look something like
https://accounts.google.com/o/oauth2/auth?
  redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;
  scope=https://www.googleapis.com/auth/devstorage.read_only&amp;
  client_id=xxxxxxx.apps.googleusercontent.com&amp;
  response_type=code
We of course replace the "xxxxxxx" in the client_id parameter value with the actual identifier for our ap- plication. We have also set the redirect_uri parameter to the reserved string 'urn:ietf:wg:oauth:2.0:oob'; this URI indicates that there is no Web page to callback to and that the application is a stand-alone/desktop ap- plication. The "oob" in this URI stands for "out of band". This means the token will be returned to us directly in the Web browser rather than the browser being redirected to a Web page specifically for our application.
Once we have created the URL string in R, we pass it to browseURL() to display this in our Web browser. This will show us the page to grant permissions, bringing us first to the login page, if necessary, of our Google account and grant the permissions. The page to grant permission will look like the screenshot dis- played in Figure 1.4 (page 16).
￼￼￼￼￼Figure 1.4. An Application Asking a User for Access
This is the Web page the user is shown when an application asks for access to the user's files. The user clicks on one of the buttons to permit or deny access.
￼￼￼￼￼Google Storage
16 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼We click on the "Allow access" button and then we are shown a page with a single text field containing the all important access token. The page will be similar to that shown in Figure 1.5 (page 17).
We then cut-and-paste this string into R and assign it to a variable, say
token = "4/Bf18Xp0_CC-NVUZGG_uVBIOaZ8IL"
Since this is just a string and it doesn't tell us anything about its purpose or content, we like to define a class for representing the string and indicating its purpose. When we forget where we got the value, we can check the class. To do this, we could use the following code:
Better yet, we can make this a Google token or even a Google Storage token with
We of course only need to define the class once and create different instances at any time.
3.1.2. Exchanging the Authorization Token for an Access To- ken
We now need to exchange the authorization token for an access token. The user is no longer involved in this negotiation which is simply between our application and Google. Of course, we are doing this in R, so we are involved but as the application. For this step, we need to use our client identifier and also its secret. We send an HTTPS POST request to https://accounts.google.com/o/oauth2/token, and include as arguments:
1. the authorization token we just received from the user, 2. the client id and client secret identifying our application,
￼￼￼Figure 1.5. Getting the Authorization Token
This is the next page the user sees in the Web browser when granting access to an application. The user copies the authorization token in the text field back to R so that the application can exchange it for an access token to be able to make requests to the API.
￼￼￼￼setClass('OAuth2PermissionToken', contains = 'character')
token = new('OAuth2PermissionToken', token)
￼setClass('GoogleOAuth2Token', contains = 'OAuth2PermissionToken')
setClass('GoogleStorageToken', contains = 'GoogleOAuth2Token')
token = new('GoogleStorageToken', token)
￼￼￼Google Storage
17 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼3. the grant_type as 'authorization_code', and
4. the redirect_uri as before ('urn:ietf:wg:oauth:2.0:oob')
We don't need or want the Web browser at this point as the user is not involved. Instead, we send the request directly from R to Google. We can use postForm() to do this with the code
The result contains the actual access token. Google returns the information in JSON format, such as
{
  "access_token" :
     "ya29.AHES6ZR9ZrB4kqY2W9iGm2fm5QNtCqaI7FJ7VTPAK3qDRQ",
  "token_type" : "Bearer",
  "expires_in" : 3600,
  "refresh_token" : "1/bVto2xxzbv_Pz8kHluGR5idfTy8qdiSFBvwIVsUEvYM"
}
Other providers may use XML or another format.
The JSON content gives us the access token and also its type (Bearer) and when it expires (in seconds). Some providers will just return these and the token will be valid forever. Most providers, however, will have the token expire after a given time, say 60 minutes as here. Rather than have to have the user grant permissions again after the access token expires, OAuth 2.0 allows the provider to return a refresh token along with the access token. The application can use this to renew the access token when it expires, without the user being involved. We'll discuss precisely how to do this later.
To get the access token from the JSON content, we can use the fromJSON() function and then save the information as an S4 class (see ??? on how to use this function). We define a class OAuth2AuthorizationToken that holds the access token, the refresh token and also the expiration time. We calculate this time by adding the value of the expires_in field to the current time Sys.time().
3.1.3. Using the Access Token in an API Request
We are now going to use the access token to access data in Google Storage. First, let's list the contents of an existing bucket named "proj1". To do this, we send an HTTPS request to https:// commondatastorage.googleapis.com/proj1 We put the access token and our client identifier into the header of the HTTP request. This looks something like
                                              Authorization
"OAuth ya29.AHES6ZR9ZrB4kqY2W9iGm2fm5QNtCqaI7FJ7VTPAK3qDRQ"
                                          x-goog-project-id
￼￼args = c(client_id = client_id,
          client_secret = client_secret,
          grant_type = 'authorization_code',
          code = as(token, "character"),
          redirect_uri = 'urn:ietf:wg:oauth:2.0:oob')
txt = postForm('https://accounts.google.com/o/oauth2/token',
                 .params = args, style = "POST")
￼￼￼￼￼Google Storage
18 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼                                             "xxxxxxxxxxxx"
                                                       Date
                          "March, 29 Mar 2012 13:11:16 PDT"
                                         x-goog-api-version
                                                        "2"
The Authorization field contains the access token in the OAuth2AuthorizationToken object that we obtained above. We prefix this access token with the string "OAuth ". The project ID is just the numbers identifying our application. We add the current date and time so that Google can verify that the request is not being "replayed" at some later time. Finally, we specify the version of the Google API to which we are sending the request. Our request could be made manually as
The result is an XML document that contains an element for each "file" in the bucket (corresponding to a folder). We can then process this and convert it to a data frame in R listing the name of the document, when it was last modified, its size in bytes, the owner's name and unique identifier.
&lt;?xml version="1.0" encoding="UTF-8"?>
&lt;ListBucketResult xmlns="http://doc.s3.amazonaws.com/2006-03-01">
  &lt;Name>proj1&lt;/Name>
  &lt;Prefix/>
  &lt;Marker/>
  &lt;IsTruncated>false&lt;/IsTruncated>
  &lt;Contents>
    &lt;Key>bar&lt;/Key>
    &lt;LastModified>2011-05-18T13:05:11.187Z&lt;/LastModified>
    &lt;ETag>"5578833a0c6cb26394a1414140718cab"&lt;/ETag>
    &lt;Size>12&lt;/Size>
    &lt;StorageClass>STANDARD&lt;/StorageClass>
    &lt;Owner>
      &lt;ID>00b4903a97f8e9...0edc6aaee&lt;/ID>
      &lt;DisplayName>Duncan Temple Lang&lt;/DisplayName>
    &lt;/Owner>
  &lt;/Contents>
  &lt;Contents>
    &lt;Key>myPlot&lt;/Key>
    &lt;LastModified>2011-05-18T12:59:26.190Z&lt;/LastModified>
    &lt;ETag>"0e49b507686b4ad978ef53832c11c157"&lt;/ETag>
    &lt;Size>14184&lt;/Size>
    &lt;StorageClass>STANDARD&lt;/StorageClass>
    &lt;Owner>
      &lt;ID>00b4903a97f8e9...edc6aaee&lt;/ID>
￼hdr = c(Authorization =
          "OAuth ya29.AHES6ZR9ZrB4kqY2W9iGm2fm5QNtCqaI7FJ7VTPAK3qDRQ"
        'x-goog-project-id' = clientID,
        Date = format(Sys.time(), "%B, %d %b %Y %H:%M:%S %Z"),
        'x-goog-api-version' =  "2")
url = "https://commondatastorage.googleapis.com/proj1"
txt = getURLContent(url, httpheader = hdr,
                    useragent = "RGoogleStorage")
￼￼￼Google Storage
19 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼      &lt;DisplayName>Duncan Temple Lang&lt;/DisplayName>
    &lt;/Owner>
  &lt;/Contents>
&lt;/ListBucketResult>
We have manually created the content for the httpheader setting in several places above. We should do this in a function since the format is the same in each of these places. The makeHeader() function in the RGoogleStorage package can be used for this task.
Let's look at an API method where we can create a document within a bucket. We specify the path of the file we want to create (or over-write) and send the request to https://commondatastorage.googleapis.com/path/ to/file. Here we are sending content from R to Google Storage. We do this via a PUT operation. We get the content of the document either from a file or an R object in memory and then upload it using the Curl options readfunction and infilesize. Again we create our own value for httpheader including the Authorization token and the client secret.
In this case, we are writing to the storage facility. For this we need write privileges to the user's Google Storage account. However, we obtained our token for reading only. This means we need to create a new permission and access token to be able to perform this operation. To do this, we repeat the steps in Section 3.1.1 (page 15). The only thing we need to change is the scope, which we set to "https:// www.googleapis.com/auth/devstorage.read_write".
We haven't covered how to send arguments in a request other than upload the contents of a file. It turns out that in the Google Storage API, this is how we pass arguments other than identifying the bucket or document in the URL of the request. For instance, to download the contents of a document in a bucket, we use the full URL to that document, e.g. https://commondatastorage.googleapis.com/proj1/abc. To get the access control list (ACL), or in other words, the permissions for a bucket, we use the "acl" parameter as part of a GET request, e.g.
&lt;duncan>I am not sure if the parameters in the request should have some sort of markup or not. I just surrounded this one in quotes. Let me know, if you would like to mark it up and I can revisit.&lt;/duncan>
where ￼ is our header including the token, client secret and date as above. ***FIXME*** When setting an ACL on a bucket, we actually upload an XML document that describes the permissions. When copying a document, we specify the target via the URL and the original document by specifying its path in the x-goog-copy-source field in the HTTP header.
3.1.4. Refreshing an OAuth2 Access Token
As we saw when we got the access token, we also received the number of seconds until the token expired and a refresh token. We put the expiration time in the OAuth2AuthorizationToken object. When we use this object in one of our functions, we should check to see if this access token has expired (by comparing the expiration time to the current time). If it has, we can use the refresh_token to obtain a new access token. This is very similar to the step we used to exchange the original user-granted token for our access token. We POST a request to https://accounts.google.com/o/oauth2/token with the client id and secret. Instead of the code and redirect_uri arguments, we specify arguments named "refresh_token" and "grant_type". For the former, we provide the value of the refresh token we were given; for the latter, we specify the string 'refresh_token'. So a request might look like
args = c(client_id = getOption("Google.storage.ID"),
￼￼getForm("https://commondatastorage.googleapis.com/phase1/foo",
           acl = "", .opts = list(httpheader = oauth2Header))
oauth2Header
￼￼￼￼￼￼￼Google Storage
20 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼￼         client_secret = getOption("Google.storage.Secret"),
         grant_type = 'refresh_token',
         refresh_token = token@refresh_token)
postForm('https://accounts.google.com/o/oauth2/token',
          .params = args, style = "POST")
This again returns a new access token and the expiration duration in JSON form and we can again turn this into an OAuth2AuthorizationToken.
We can encapsulate getting and refreshing a token into a function so that the details are hidden from the caller. The getAuth() function in the RGoogleStorage package does this and the function getPermis- sion() takes care of making the initial request and guiding the caller to the Web browser to grant the permissions.
We should write our functions for API methods so that they check the expiration and update the OAuth2AuthorizationToken as necessary. Unfortunately, if any of these functions update the token, then that function may find it difficult to return the token as part of the regular value returned from the API method called. For example, suppose we have a function download() to retrieve a document from Google Storage. This function should return the contents of the document, but if it refreshes the token, that new access token will be discarded when the function returns. This is fine as the other functions can also refresh an expired token when they are called, but there may be a lot of unnecessary refresh requests. Instead, we could issue a warning and ask the user to explicitly refresh the token before the next function call. However, an alternative is to use a mutable object to represent the token and refresh token, i.e. the OAuth2AuthorizationToken. We would pass this mutable object to our functions which would re- fresh them if necessary. Since they update that mutable object, the new token and expiration time are in the original object passed to the function and there is no need for the function to return this additional informa- tion. This is an example where using a reference class is appropriate simply because we cannot easily return the updated token as well as the actual result of a function.
4. Further Reading
&lt;duncan>Did you want to write a short paragraph as to why you are pointing the reader to these various sources and which parts are particularly worth paying attention to?&lt;/duncan>
There are several good books and online documents that describe OAuth 1.0 and OAuth 2.0.
[1] Eran Hammer. 2011. The OAuth 1.0 Guide. http://hueniverse.com/oauth/guide
This guide is a terrific explanation of OAuth 1.0 written in a manner that explains it to users of OAuth in very clear terms. It is written by the primary author of the OAuth 1.0 specification. The guide explains all the details of OAuth 1.0, from high-level concepts to details about how to sign each request. There is even an interactive document to illustrate all the steps of the signing process for those who need to understand this.
[2] Johnathan LeBlanc. 2011. Programming Social Applications. Building Viral Experiences with OpenSo- cial, OAuth, OpenID, and Distributed Web Frameworks. O'Reilly Media / Yahoo Press. Sebastopol CA. 978-1449394912.
Chapter 9 of this book provides a very readable description of OAuth 1.0 and also OAuth 2.0. If the discus- sion at hueniverse.com is not entirely clear, this book should help to clarify those difficult concepts from a different perspective. The book is also interesting for other topics it covers.
￼￼￼￼￼￼￼Further Reading
21 2012-05-29T11:08:39-07:00
￼￼Authentication with Web Services via OAuth
￼[4] . Using OAuth 2.0 to Access Google APIs. https://developers.google.com/accounts/docs/OAuth2
Google has a good overview of OAuth 2.0 and how to use it. This is quite clear and comprehensive.
[5] Eran Hammer. 2010. OAuth 1.0 Specification. Internet Engineering Task Force (IETF). http:// tools.ietf.org/html/rfc58492070-1721.
This is the official specification for OAuth 1.0. As such it is a little more pedantic than explanatory. It is a good reference when writing the code to implement OAuth 1.0.
￼￼￼Further Reading
22 2012-05-29T11:08:39-07:00
</Text>
        </Document>
        <Document ID="6">
            <Title>How R fits in</Title>
            <Text>R is great because it is,
* Open source. Anyone can contribute code that is avaiable for anyone else to run freely. There is no issue with people requiring expesive proporietary software.
* If a set of scripts become useful as a package, then it can easily be made available during development to anyone else while in progress (using devtools). But once it is complete, has been tested, and has received adequate feedback, it can then be shared with everyone else.

========================</Text>
        </Document>
        <Document ID="13">
            <Title>todos</Title>
            <Text>Email Andrew about vizuallity
Find heather piwowar’s paper on github
Find Carl’s examples for using rcitations.
</Text>
        </Document>
        <Document ID="7">
            <Title>ropensci</Title>
            <Text>
In an effort to make these tools in R a bit more formal, two colleagues (Carl, Scott) and I began an organization called RopenSci. Short for R open science. Our aim is to build tools we might use to support our own academic interests (i.e. Gain access to biological data and also literature, embed these in reproducible code, and make the entire process transparent from start to finish).


Caveats: 
A) we do realize that this level of openness isn’t something that most academics could go given intense competition. In that case, providing access post-hoc or after a certain embargo time would be the appropriate way to go.
B) While people might want to retain exclusive access to derive more our of their generated data, both the funders and data repositories provide ways to protect them from full-public access for various lengths of time during which something as simple as metadata becomes available and the authors can work out a different arrangement with anyone wishing to use their data.

However, such a level of openness does increase the level of credibility and makes the entire process more accessible to skeptical people in the public.
</Text>
        </Document>
        <Document ID="18">
            <Title>journal.pone.0000308</Title>
            <Text>￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼Sharing Detailed Research Data Is Associated with
Increased Citation Rate
Heather A. Piwowar*, Roger S. Day, Douglas B. Fridsma
Department of Biomedical Informatics, University of Pittsburgh School of Medicine, Pittsburgh, Pennsylvania, United States of America
Background. Sharing research data provides benefit to the general scientific community, but the benefit is less obvious for the investigator who makes his or her data available. Principal Findings. We examined the citation history of 85 cancer microarray clinical trial publications with respect to the availability of their data. The 48% of trials with publicly available microarray data received 85% of the aggregate citations. Publicly available data was significantly (p = 0.006) associated with a 69% increase in citations, independently of journal impact factor, date of publication, and author country of origin using linear regression. Significance. This correlation between publicly available data and increased literature impact may further motivate investigators to share their detailed research data.
Citation: Piwowar HA, Day RS, Fridsma DB (2007) Sharing Detailed Research Data Is Associated with Increased Citation Rate. PLoS ONE 2(3): e308. doi:10.1371/journal.pone.0000308
INTRODUCTION
Sharing information facilitates science. Publicly sharing detailed research data–sample attributes, clinical factors, patient outcomes, DNA sequences, raw mRNA microarray measurements–with other researchers allows these valuable resources to contribute far beyond their original analysis[1]. In addition to being used to confirm original results, raw data can be used to explore related or new hypotheses, particularly when combined with other publicly available data sets. Real data is indispensable when investigating and developing study methods, analysis techniques, and software implementations. The larger scientific community also benefits: sharing data encourages multiple perspectives, helps to identify errors, discourages fraud, is useful for training new researchers, and increases efficient use of funding and patient population resources by avoiding duplicate data collection.
Believing that that these benefits outweigh the costs of sharing research data, many initiatives actively encourage investigators to make their data available. Some journals, including the PLoS family, require the submission of detailed biomedical data to publicly available databases as a condition of publication[2–4]. Since 2003, the NIH has required a data sharing plan for all large funding grants. The growing open-access publishing movement will perhaps increase peer pressure to share data.
However, while the general research community benefits from shared data, much of the burden for sharing the data falls to the study investigator. Are there benefits for the investigators themselves?
A currency of value to many investigators is the number of times their publications are cited. Although limited as a proxy for the scientific contribution of a paper[5], citation counts are often used in research funding and promotion decisions and have even been assigned a salary-increase dollar value[6]. Boosting citation rate is thus is a potentially important motivator for publication authors.
In this study, we explored the relationship between the citation rate of a publication and whether its data was made publicly available. Using cancer microarray clinical trials, we addressed the following questions: Do trials which share their microarray data receive more citations? Is this true even within lower profile trials? What other data-sharing variables are associated with an increased citation rate? While this study is not able to investigate causation, quantifying associations is a valuable first step in understanding these relationships. Clinical microarray data provides a useful environment for the investigation: despite being valuable for reuse and extremely costly to collect, is not yet universally shared.
RESULTS
We studied the citations of 85 cancer microarray clinical trials published between January 1999 and April 2003, as identified in a systematic review by Ntzani and Ioannidis[7] and listed in Supplementary Text S1. We found 41 of the 85 clinical trials (48%) made their microarray data publicly available on the internet. Most data sets were located on lab websites (28), with a few found on publisher websites (4), or within public databases (6 in the Stanford Microarray Database (SMD)[8], 6 in Gene Expression Omnibus (GEO)[9], 2 in ArrayExpress[10], 2 in the NCI GeneExpression Data Portal (GEDP)(gedp.nci.nih.gov); some datasets in more than one location). The internet locations of the datasets are listed in Supplementary Text S2. The majority of datasets were made available concurrently with the trial publication, as illustrated within the WayBackMachine internet archives (www.archive.org/web/web.php) for 25 of the datasets and mention of supplementary data within the trial publication itself for 10 of the remaining 16 datasets. As seen in Table 1, trials published in high impact journals, prior to 2001, or with US authors were more likely to share their data.
The cohort of 85 trials was cited an aggregate of 6239 times in 2004–2005 by 3133 distinct articles (median of 1.0 cohort citation per article, range 1–23). The 48% of trials which shared their data received a total of 5334 citations (85% of aggregate), distributed as shown in Figure 1.
Academic Editor: John Ioannidis, University of Ioannina School of Medicine, Greece
Received December 13, 2006; Accepted February 26, 2007; Published March 21, 2007
Copyright: ß 2007 Piwowar et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
Funding: HAP was supported by NLM Training Grant Number 5T15-LM007059-19. The NIH had no role in study design, data collection or analysis, writing the paper, or the decision to submit it for publication. The publication contents are solely the responsibility of the authors and do not necessarily represent the official views of the NIH.
Competing Interests: The authors have declared that no competing interests exist.
* To whom correspondence should be addressed. E-mail: hpiwowar@cbmi.pitt. edu
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼PLoS ONE | www.plosone.org 1
March 2007 | Issue 3 | e308
Sharing Data Citation Rate
￼Table 1. Characteristics of Eligible Trials by Data Sharing.
..................................................................................................................................................
￼￼￼Number of Articles
Total Data Shared
Data Not Shared 44 (52%)
0 (0%) 44 (60%) 1 (17%) 43 (54%) 21 (38%) 23 (79%)
Odds Ratio (95% confidence interval)
‘ (3.8 to ‘)
6.0 (0.6 to 288.5)
6.4 (2.0 to 21.9)
￼￼￼￼￼￼￼￼TOTAL 85 High Impact (. = 25) 12 Low Impact Journal 73 Published 1999–2000 6 Published 2001–2003 79 Include a US Author 56 No US Authors 29
doi:10.1371/journal.pone.0000308.t001
41 (48%)
12 (100%) 29 (40%) 5 (83%) 36 (46%) 35 (63%) 6 (21%)
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼Whether a trial’s dataset was made publicly available was significantly associated with the log of its 2004–2005 citation rate (69% increase in citation count; 95% confidence interval: 18 to 143%, p = 0.006), independent of journal impact factor, date of publication, and US authorship. Detailed results of this multivar- iate linear regression are given in Table 2. A similar result was found when we regressed on the number of citations each trial received during the 24 months after its publication (45% increase in citation count; 95% confidence interval: 1 to 109%, p = 0.050).
To confirm that these findings were not dependent on a few extremely high-profile papers, we repeated our analysis on a subset of the cohort. We define papers published after the year 2000 in journals with an impact factor less than 25 as lower-profile publications. Of the 70 trials in this subset, only 27 (39%) made their data available, although they received 1875 of 2761 (68%) aggregate citations. The distribution of the citations by data availability in this subset is shown in Figure 2. The association between data sharing and citation rate remained significant in this
Figure 1. Distribution of 2004–2005 citation counts of 85 trials by data availability. The 41 clinical trial publications which publicly shared their microarray data received more citations, in general, than the 44 publications which did not share their microarray data. In this plot of the distribution of citation counts received by each publication, the extent of the box encompasses the interquartile range of the citation counts, whiskers extend to 1.5 times the interquartile range, and lines within the boxes represent medians. doi:10.1371/journal.pone.0000308.g001
lower-profile subset, independent of other covariates within a multivariate linear regression (71% increase in citation count; 95% confidence interval: 19 to 146%, p = 0.005).
Lastly, we performed exploratory analysis on citation rate within the subset of trials which shared their microarray data; results are given in Table 3 and raw covariate data in Supplementary Data S1. The number of patients in a trial and a clinical endpoint correlated with increased citation rate. Assuming shared data is actually re- analyzed, one might expect an increase in citations for those trials which generated data on a standard platform (Affymetrix), or released it in a central location or format (SMD, GEO, GEDP)[11]. However, the choice of platform was insignificant and only those trials located in SMD showed a weak trend of increased citations. In fact, the 6 trials with data in GEO (in addition to other locations for 4 of the 6) actually showed an inverse relationship to citation rate, though we hesitate to read much into this due to the small number of trials in this set. The few trials in this cohort which, in addition to gene expression fold-change or other preprocessed information, shared their raw probe data or actual microarray images did not receive additional citations. Finally, although finding diverse microarray datasets online is non-trivial, an additional increase in citations was not noted for trials which mentioned their Supple- mentary Material within their paper, nor for those trials with datasets identified by a centralized, established data mining website. In summary, only trial design features such as size and clinical endpoint showed a significant association with citation rate; covariates relating to the data collection and how the data was made available only showed very weak trends. Perhaps with a larger and more balanced sample of trials with shared data these trends would be more clear.
Table 2. Multivariate regression on citation count for 85 publications
￼￼......................................................................
￼￼￼￼￼￼Publish in a journal with twice the impact factor
Increase the publication date by a month Include a US author
Make data publicly available
Percent increase in citation count (95% confidence interval)
84% (59 to 109%)
23% (25 to 22%) 38% (1 to 89%) 69% (18 to 143%)
p-value
,0.001 ,0.001
0.049
0.006
￼￼￼￼￼We calculated a multivariate linear regression over the citation counts, including covariates for journal impact factor, date of publication, US authorship, and data availability. The coefficients and p-values for each of the covariates are shown here, representing the contribution of each covariate to the citation count, independent of other covariates. doi:10.1371/journal.pone.0000308.t002
￼PLoS ONE | www.plosone.org 2
March 2007 | Issue 3 | e308
...............................................
.......................................................
Sharing Data Citation Rate
￼Figure 2. Distribution of 2004–2005 citation counts of the 70 lower- profile trials by data availability. For trials which were published after 2000 and in journals with an impact factor less than 25, the 27 clinical trial publications which publicly shared their microarray data received more citations, in general, than the 43 publications which did not share their microarray data. In this plot of the distribution of citation counts received by each publication, the extent of the box encompasses the interquartile range of the citation counts, whiskers extend to 1.5 times the interquartile range, and lines within the boxes represent medians. doi:10.1371/journal.pone.0000308.g002
DISCUSSION
We found that cancer clinical trials which share their microarray data were cited about 70% more frequently than clinical trials which do not. This result held even for lower-profile publications and thus is relevant to authors of all trials.
A parallel can be drawn between making study data publicly available and publishing a paper itself in an open-access journal. The association with an increased citation rate is similar[12]. While altruism no doubt plays a part in the motivation of authors in both cases, studies have found that an additional reason authors choose to publish in open-access journals is that they believe their articles will be cited more frequently[13,14], endorsing the relevance of our result as a potential motivator.
We note an important limitation of this study: the demonstrated association does not imply causation. Receiving many citations and sharing data may stem from a common cause rather than being directly causally related. For example, a large, high-quality, clinically important trial would naturally receive many citations due to its medical relevance; meanwhile, its investigators may be more inclined to share its data than they would be for a smaller trial-perhaps due greater resources or confidence in the results.
Nonetheless, if we speculate for a moment that some or all of the association is indeed causal, we can hypothesize several mechan- isms by which making data available may increase citations. The simplest mechanism is due to increased exposure: listing the dataset in databases and on websites will increase the number of people who encounter the publication. These people may then subsequently cite it for any of the usual reasons one cites a paper, such as paying homage, providing background reading, or noting corroborating or disputing claims ([15] provides a summary of research into citation behavior). More interestingly, evidence suggests that shared microarray data is indeed often reana- lyzed[16], so at least some of the additional citations are certainly in this context. Finally, these re-analyses may spur enthusiasm and synergy around a specific research question, indirectly focusing publications and increasing the citation rate of all participants. These hypotheses are not tested in this study: additional research is needed to study the context of these citations and the degree, variety, and impact of any data re-use. Further, it would be interesting to assess the impact of reuse on the community, quantifying whether it does in fact lead to collaboration, a reduction in resource use, and scientific advances.
Since it is generally agreed that sharing data is of value to the scientific community[16–21], it is disappointing that less than half of the trials we looked at made their data publicly available. It is possible that attitudes may have changed in the years since these trials were published, however even recent evidence (in a field tangential to microarray trials) demonstrates a lack of willingness and ability to share data: an analysis in 2005 by Kyzas et al.[22] found that primary investigators for 17 of 63 studies on TP53 status in head and neck squamous cell carcinoma did not respond to a request for additional information, while 5 investigators replied they were unable to retrieve raw data.
Indeed, there are many personal difficulties for those who undertake to share their data[1]. A major cost is time: the data have to be formatted, documented, and released. Unfortunately this investment is often larger than one might guess: in the realm of microarray and particularly clinical information, it is nontrivial to
￼Table 3. Exploratory regressions on citation count for the 41 publications with shared data
..................................................................................................................................................
￼￼￼Number of articles (% of total) Number of citations (% of total) Percent increase in citation count p-value
￼￼￼￼￼￼￼￼￼￼TOTAL ￼ ￼ ￼ 41 ￼ 5334
￼￼￼Trial size.25 patients 26 (63%) 3704 (69%) 122% ,0.001
￼￼￼￼￼Clinical endpoint ￼ ￼ ￼ 18 (44%) ￼ 3404 (64%) ￼ 79% ￼ 0.01
Affymetrix platform 22 (54%) ￼ ￼ 2735 (51%) ￼ 18% ￼ 0.43
￼￼￼In GEO database 6 (15%) 939 (18%) 252% 0.02
￼￼￼￼￼￼In SMD database 6 (15%) 1114 (21%) 24% 0.48
￼￼￼￼￼Raw data available 20 (49%) ￼ ￼ 2437 (46%) ￼ 22% ￼ 0.91
￼￼Pub mentions Suppl. Data 35 (85%) ￼ ￼ ￼ 4854 (91%) ￼ 11% ￼ 0.73
￼￼Has Oncomine profile 35 (85%) 4884 (92%) 19% 0.54
The coefficient and p-value for each covariate in the table were calculated from separate multivariate linear regressions over the citation count, including covariates for journal impact factor, date of publication, and US authorship.
doi:10.1371/journal.pone.0000308.t003
￼￼￼PLoS ONE | www.plosone.org 3 March 2007 | Issue 3 | e308
.......................................................
decide what data to release, how to de-identify it, how to format it, and how to document it. Further, it is sometimes complicated to decide where to best publish data, since supplementary in- formation and laboratory sites are transient[23,24] Beyond a time investment, releasing data can induce fear. There is a possibility that the original conclusions may be challenged by a re-analysis, whether due to possible errors in the original study[25], a misunderstanding or misinterpretation of the data[26], or simply more refined analysis methods. Future data miners might discover additional relationships in the data, some of which could disrupt the planned research agenda of the original investigators. Investigators may fear they will be deluged with requests for assistance, or need to spend time reviewing and possibly rebutting future re-analyses. They might feel that sharing data decreases their own competitive advantage, whether future publishing opportunities, information trade-in-kind offers with other labs, or potentially profit-making intellectual property. Finally, it can be complicated to release data. If not well-managed, data can become disorganized and lost. Some informed consent agreements may not obviously cover subsequent uses of data. De-identification can be complex. Study sponsors, particularly from industry, may not agree to release raw detailed information. Data sources may be copyrighted such that the data subsets can not be freely shared, though it is always worth asking.
Although several of these difficulties are challenging to overcome, many are being addressed by a variety of initiatives, thereby decreasing the barriers to data sharing. For example, within the area of microarray clinical trials, several public microarray databases (SMD[27], GEO[9], ArrayExpress[10], CIBEX[28], GEDP(gedp.nci.nih.gov)) offer an obvious, central- ized, free, and permanent data storage solution. Standards have been developed to specify minimal required data elements (MIAME[29] for microarray data, REMARK[30] for prognostic study details), consistent data encoding (MAGE-ML[31] for microarray data), and semantic models (BRIDG (www.bridgpro- ject.org) for study protocol details). Software exists to help de- identify some types of patient records (De-ID[32]). The NIH and other agencies allow funds for data archiving and sharing. Finally, large initiatives (NCI’s caBIG[33]) are underway to build tools and communities to enable and advance sharing data.
Research consumes considerable resources from the public trust. As data sharing gets easier and benefits are demonstrated for the individual investigator, hopefully authors will become more apt to share their study data and thus maximize its usefulness to society.
In the spirit of this analysis, we have made publicly available the bibliometric detailed research data compiled for this study (see Supplementary Information and http://www.pitt.edu/,hap7).
MATERIALS AND METHODS
Identification and Eligibility of Relevant Studies
We compared the citation impact of clinical trials which made their cancer microarray data publicly available to the citation impact of trials which did not. A systematic review by Ntzani and Ioannidis[7] identified clinical trials published between January 1999 and April 2003 which investigated correlations between microarray gene expression and human cancer outcomes and correlates. We adopted this set of 85 trials as the cohort of interest.
Data Extraction
We assessed whether each of these trials made its microarray data publicly available by examining a variety of publication and internet resources. Specifically, we looked for mention of
Supplementary Information within the trial publication, searched the Stanford Microarray Database (SMD)[8], Gene Expression Omnibus (GEO)[9], ArrayExpress[10], CIBEX[28], and the NCI GeneExpression Data Portal (GEDP)(gedp.nci.nih.gov), investi- gated whether a data link was provided within Oncomine[34], and consulted the bibliography of data re-analyses. Microarray data release was not required by any journals within the timeframe of these trial publications. Some studies may make their data available upon individual request, but this adds a burden to the data user and so was not considered ‘‘publicly available’’ for the purposes of this study.
We attempted to determine the date data was made available through notations in the published paper itself and records within the WayBackMachine internet archive (www.archive.org/web/ web.php). Inclusion in the WayBackMachine archive for a given date proves a resource was available, however, because archiving is not comprehensive, absence from the archive does not itself demonstrate a resource did not exist on that date.
The citation history for each trial was collected through the Thomson Scientific Institute for Scientific Information (ISI) Science Citation Index at the Web of Science Database (www. isinet.com). Only citations with a document type of ‘Article’ were considered, thus excluding citations by reviews, editorials, and other non-primary research papers.
For each trial, we also extracted the impact factor of the publishing journal (ISI Journal Citation Reports 2004), the date of publication, and the address of the authors from the ISI Web of Science. Trial size, clinical endpoint, and microarray platform were extracted from the Ntzani and Ioannidis review[7].
Analysis
The main analyses addressed the number of citations each trial received between January 2004 and December 2005. Because the pattern of citations rates is complex–changing not only with duration since publication but also with maturation of the general microarray field–a confirmatory analysis was performed using the number of citations each publication received within the first 24 months of its publication.
Although citation patterns covering a broad scope of literature types are left-skewed[35], we verified that citation rates within our relatively homogeneous cohort were roughly log-normal and thus used parametric statistics.
Multivariate linear regression was used to evaluate the association between the public availability of a trial’s microarray data and number of citations (after log transformation) it received. The impact factor of the journal which published each trial, the date of publication, and the country of authors are known to correlate to citation rate[36], so these factors were included as covariates. Impact factor was log-transformed, date of publication was measured as months since January 1999, and author country was coded as 1 if any investigator has a US address and 0 otherwise.
Since seminal papers–often those published early in the history a field or in very high-impact journals–receive an unusually high number of citations, we performed a subset analysis to determine whether our results held when considering only those trials which were published after 2000 and in lower-impact (,25) journals.
Finally, as exploratory analysis within the subset of all trials with publicly available microarray data, we looked at the linear regression relationships between additional covariates and citation count. Covariates included trial size, clinical endpoint, microarray platform, inclusion in various public databases, release of raw data, mention of supplementary information, and reference within the Oncomine[34] repository.
Sharing Data Citation Rate
￼PLoS ONE | www.plosone.org 4
March 2007 | Issue 3 | e308
Found at: doi:10.1371/journal.pone.0000308.s003 (0.01 MB version 2.1[37]; the code is included as Supplementary Text S3. P- TXT)
Statistical analysis was performed using the stats package in R values are two-tailed.
Data S1 Raw Citation Counts and Covariates
Found at: doi:10.1371/journal.pone.0000308.s004 (0.04 MB SUPPORTING INFORMATION XLS)
Text S1 Cohort Publication Bibliography
Found at: doi:10.1371/journal.pone.0000308.s001 (0.05 MB DOC)
Text S2 Locations of Publicly Available Data for the Cohort Found at: doi:10.1371/journal.pone.0000308.s002 (0.05 MB DOC)
Text S3 Statistical Analysis R-code REFERENCES
1. Fienberg SE, Martin ME, Straf ML (1985) Sharing research data. Washington, D.C.: National Academy Press. viii, 225 p.
2. (1987) A new system for direct submission of data to the nucleotide sequence data banks. Nucleic Acids Research 15: front matter.
3. McCain KW (1995) Mandating Sharing: Journal Policies in the Natural Sciences. Science Communication 16: 403–431.
4. (2003) Microarray policy. Nat Immunol 4: 93.
5. Seglen PO (1997) Why the impact factor of journals should not be used for
evaluating research. Bmj 314: 498–502.
6. Diamond AM, Jr. (1986) What is a Citation Worth? The Journal of Human
Resources 21: 200–215.
7. Ntzani EE, Ioannidis JP (2003) Predictive ability of DNA microarrays for cancer
outcomes and correlates: an empirical assessment. Lancet 362: 1439–1444.
8. Sherlock G, Hernandez-Boussard T, Kasarskis A, Binkley G, Matese JC, et al. (2001) The Stanford Microarray Database. Nucleic Acids Res 29: 152–155.
9. Edgar R, Domrachev M, Lash AE (2002) Gene Expression Omnibus: NCBI
gene expression and hybridization array data repository. Nucleic Acids Res 30:
207–210.
10. Parkinson H, Sarkans U, Shojatalab M, Abeygunawardena N, Contrino S, et al.
(2005) ArrayExpress–a public repository for microarray gene expression data at
the EBI. Nucleic Acids Res 33: D553–555.
11. Brazma A, Robinson A, Cameron G, Ashburner M (2000) One-stop shop for
microarray data. Nature 403: 699–700.
12. Antelman K (2004) Do Open Access Articles Have a Greater Research Impact?
College and Research Libraries 65: 372–382.
13. Swan A, Brown S (2004) Authors and open access publishing. Learned
Publishing 17: 219–224.
14. Eysenbach G (2006) Citation advantage of open access articles. PLoS Biol 4:
e157.
15. Case DO, Higgins GM (2000) How can we investigate citation behavior? A
study of reasons for citing literature in communication. Journal of the American
Society for Information Science 51: 635–645.
16. Ventura B (2005) Mandatory submission of microarray data to public
repositories: how is it working? Physiol Genomics 20: 153–156.
17. Theologis A, Davis RW (2004) To give or not to give? That is the question. Plant
Physiol 135: 4–9.
18. Cech T (2003) Sharing Publication-Related Data and Materials: Responsibilities
of Authorship in the Life Sciences. Washington: National Academies Press.
19. Popat S, Houlston RS (2005) Re: Reporting recommendations for tumor marker prognostic studies (REMARK). J Natl Cancer Inst 97: 1855; author reply 1855–
1856.
20. Ball CA, Sherlock G, Brazma A (2004) Funding high-throughput data sharing.
Nat Biotechnol 22: 1179–1183.
ACKNOWLEDGMENTS Author Contributions
Conceived and designed the experiments: HP. Performed the experiments: HP. Analyzed the data: HP. Wrote the paper: HP. Other: Reviewed the data analysis and interpretation, reviewed the paper: RD Discussed the study motivation and scope, reviewed the paper: DF.
21.
22. 23. 24.
25. 26.
27.
28. 29.
30.
31.
32.
33. 34.
35.
36. 37.
Riley RD, Abrams KR, Sutton AJ, Lambert PC, Jones DR, et al. (2003) Reporting of prognostic markers: current problems and development of guidelines for evidence-based practice in the future. Br J Cancer 88: 1191–1198. Kyzas PA, Loizou KT, Ioannidis JP (2005) Selective reporting biases in cancer prognostic factor studies. J Natl Cancer Inst 97: 1043–1055.
Santos C, Blake J, States DJ (2005) Supplementary data need to be kept in public repositories. Nature 438: 738.
Evangelou E, Trikalinos TA, Ioannidis JP (2005) Unavailability of online supplementary scientific information from articles published in major journals. Faseb J 19: 1943–1944.
Check E (2004) Proteomics and cancer: Running before we can walk? Nature 429: 496.
Liotta LA, Lowenthal M, Mehta A, Conrads TP, Veenstra TD, et al. (2005) Importance of communication between producers and consumers of publicly available experimental data. J Natl Cancer Inst 97: 310–314.
Ball CA, Awad IA, Demeter J, Gollub J, Hebert JM, et al. (2005) The Stanford Microarray Database accommodates additional microarray platforms and data formats. Nucleic Acids Res 33: D580–582.
Ikeo K, Ishi-i J, Tamura T, Gojobori T, Tateno Y (2003) CIBEX: center for information biology gene expression database. C R Biol 326: 1079–1082. Brazma A, Hingamp P, Quackenbush J, Sherlock G, Spellman P, et al. (2001) Minimum information about a microarray experiment (MIAME)-toward standards for microarray data. Nat Genet 29: 365–371.
McShane LM, Altman DG, Sauerbrei W, Taube SE, Gion M, et al. (2005) Reporting recommendations for tumor marker prognostic studies (REMARK). J Natl Cancer Inst 97: 1180–1184.
Spellman PT, Miller M, Stewart J, Troup C, Sarkans U, et al. (2002) Design and implementation of microarray gene expression markup language (MAGE-ML). Genome Biol 3: RESEARCH0046.
Gupta D, Saul M, Gilbertson J (2004) Evaluation of a deidentification (De-Id) software engine to share pathology reports and clinical documents for research. Am J Clin Pathol 121: 176–186.
Buetow KH (2005) Cyberinfrastructure: empowering a ‘‘third way’’ in biomedical research. Science 308: 821–824.
Rhodes DR, Yu J, Shanker K, Deshpande N, Varambally R, et al. (2004) ONCOMINE: a cancer microarray database and integrated data-mining platform. Neoplasia 6: 1–6.
Weale AR, Bailey M, Lear PA (2004) The level of non-citation of articles within a journal as a measure of quality: a comparison to the impact factor. BMC Med Res Methodol 4: 14.
Patsopoulos NA, Analatos AA, Ioannidis JP (2005) Relative citation impact of various study designs in the health sciences. Jama 293: 2362–2366.
R Development Core Team (2004) R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing.
Sharing Data Citation Rate
￼PLoS ONE | www.plosone.org 5
March 2007 | Issue 3 | e308
</Text>
        </Document>
        <Document ID="8">
            <Title>RMendeley</Title>
        </Document>
        <Document ID="14">
            <Title>Outline from wf</Title>
            <Text>My BARUG talk
	•	Introduce myself
	•	My connection to R + open science
	•	What is Open Science?
	•	How R fits in?
	•	Tools are free and can become instantly available to anyone anywhere.
	•	CRAN pre-dates the app store and can easily disseminate code.
	•	What is lacking?
	•	R tools - introduce ropensci.org
	•	RMendeley (citation processing).
	•	DataONE
	•	The possibility of pushing data into the cloud from R.
	•	rProvenance
	•	Ritis
	•	rgbif
	•	Dryad
	•	Total Impact
	•	Licenses -- we choose CC0, the first ever for an R package making these tools completely free.
	•	These are all use-cases for connecting R with widely available data.
	•	Anything with an API
	•	anything that works with oauth1 or oauth2
	•	Mapping
	•	CartoDB
	•	Encapsulate the entire research process in R.
	•	This includes report generation.
	•	Open CPU.org 
	•	Reproducible research and the executable paper
	•	Peer review process.
Created with WorkFlowy.com
</Text>
        </Document>
        <Document ID="9">
            <Title>RDryad</Title>
            <Text>Details from the Dryad repository

Ecology, and various other journals in my field have begun requiring their authors to deposit data into this repository. We provide programmatic access to these data via R. 

</Text>
        </Document>
    </Documents>
</SearchIndexes>